{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geometric_learning_final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WoI2KClxM6d9"
      ],
      "authorship_tag": "ABX9TyNP3QOCZ2snoI44t4h/U7if",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitshmidov/geometric_learning_project/blob/main/geometric_learning_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_a7zJA-iDrw"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0REbDcKoc_3",
        "outputId": "cb15ec43-8ac2-4391-da10-8098ded328b4"
      },
      "source": [
        "!pip install pyvista\n",
        "!pip install trimesh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvista\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/f3/e95c0b88d2c7e8cf84e49252cf72da4cf4afce310cea573c7b2997a99823/pyvista-0.28.1-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from pyvista) (2.4.1)\n",
            "Collecting vtk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/f5/1971e21b7da2875fa07754e49c302cdb140de4c8585d56df8027d08ce299/vtk-9.0.1-cp37-cp37m-manylinux2010_x86_64.whl (103.4MB)\n",
            "\u001b[K     |████████████████████████████████| 103.4MB 49kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyvista) (1.19.5)\n",
            "Collecting transforms3d==0.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/f7/e85809168a548a854d7c1331560c27b4f5381698d29c12e57759192b2bc1/transforms3d-0.3.1.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pyvista) (1.4.4)\n",
            "Collecting scooby>=0.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/50/d4/4edc294b2808d67057ffbeb7cefebda17e98392353de7be9c28300f9a331/scooby-0.5.6-py3-none-any.whl\n",
            "Collecting meshio<5.0,>=4.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/61/d6aa24e10d110ec86acf11cd06a54fd94d40c33af9979d75e900da6ebbcc/meshio-4.3.10-py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pyvista) (7.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from meshio<5.0,>=4.0.3->pyvista) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->meshio<5.0,>=4.0.3->pyvista) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->meshio<5.0,>=4.0.3->pyvista) (3.7.4.3)\n",
            "Building wheels for collected packages: transforms3d\n",
            "  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transforms3d: filename=transforms3d-0.3.1-cp37-none-any.whl size=59374 sha256=031ce72a886044e9114512a098d2cea367ef8a52015daa0f740081b66873585a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/3c/84/28d36677f3c760c048bd02b5a547ea0c4027770cc9cdb9af1e\n",
            "Successfully built transforms3d\n",
            "Installing collected packages: vtk, transforms3d, scooby, meshio, pyvista\n",
            "Successfully installed meshio-4.3.10 pyvista-0.28.1 scooby-0.5.6 transforms3d-0.3.1 vtk-9.0.1\n",
            "Collecting trimesh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5b/b971412e6be76e4745973002d2a90ff7e9ed084a363aac690d8e4ed4b2a4/trimesh-3.9.7-py3-none-any.whl (629kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trimesh) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from trimesh) (53.0.0)\n",
            "Installing collected packages: trimesh\n",
            "Successfully installed trimesh-3.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xChOBbx4nStU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a29a4c-f3e4-4c3c-b225-9c93656ad7ea"
      },
      "source": [
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import pyvista as pv\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import trimesh\n",
        "from tqdm.notebook import tqdm\n",
        "from trimesh.sample import sample_surface_even\n",
        "\n",
        "from typing import Tuple\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoI2KClxM6d9"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spcm39HPNAZC"
      },
      "source": [
        "def _variable_on_cpu(name, shape, initializer, use_fp16=False):\n",
        "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
        "  Args:\n",
        "    name: name of the variable\n",
        "    shape: list of ints\n",
        "    initializer: initializer for Variable\n",
        "  Returns:\n",
        "    Variable Tensor\n",
        "  \"\"\"\n",
        "  with tf.device('/cpu:0'):\n",
        "    dtype = tf.float16 if use_fp16 else tf.float32\n",
        "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
        "  return var\n",
        "\n",
        "\n",
        "def _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):\n",
        "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
        "\n",
        "  Note that the Variable is initialized with a truncated normal distribution.\n",
        "  A weight decay is added only if one is specified.\n",
        "\n",
        "  Args:\n",
        "    name: name of the variable\n",
        "    shape: list of ints\n",
        "    stddev: standard deviation of a truncated Gaussian\n",
        "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "        decay is not added for this Variable.\n",
        "    use_xavier: bool, whether to use xavier initializer\n",
        "\n",
        "  Returns:\n",
        "    Variable Tensor\n",
        "  \"\"\"\n",
        "  if use_xavier:\n",
        "    initializer = tf.contrib.layers.xavier_initializer()\n",
        "  else:\n",
        "    initializer = tf.truncated_normal_initializer(stddev=stddev)\n",
        "  var = _variable_on_cpu(name, shape, initializer)\n",
        "  if wd is not None:\n",
        "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "    tf.add_to_collection('losses', weight_decay)\n",
        "  return var\n",
        "\n",
        "\n",
        "def conv1d(inputs,\n",
        "          num_output_channels,\n",
        "          kernel_size,\n",
        "          scope,\n",
        "          stride=1,\n",
        "          padding='SAME',\n",
        "          use_xavier=True,\n",
        "          stddev=1e-3,\n",
        "          weight_decay=0.0,\n",
        "          activation_fn=tf.nn.relu,\n",
        "          bn=False,\n",
        "          bn_decay=None,\n",
        "          is_training=None):\n",
        "  \"\"\" 1D convolution with non-linear operation.\n",
        "\n",
        "  Args:\n",
        "    inputs: 3-D tensor variable BxLxC\n",
        "    num_output_channels: int\n",
        "    kernel_size: int\n",
        "    scope: string\n",
        "    stride: int\n",
        "    padding: 'SAME' or 'VALID'\n",
        "    use_xavier: bool, use xavier_initializer if true\n",
        "    stddev: float, stddev for truncated_normal init\n",
        "    weight_decay: float\n",
        "    activation_fn: function\n",
        "    bn: bool, whether to use batch norm\n",
        "    bn_decay: float or float tensor variable in [0,1]\n",
        "    is_training: bool Tensor variable\n",
        "\n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    num_in_channels = inputs.get_shape()[-1].value\n",
        "    kernel_shape = [kernel_size,\n",
        "                    num_in_channels, num_output_channels]\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                        shape=kernel_shape,\n",
        "                                        use_xavier=use_xavier,\n",
        "                                        stddev=stddev,\n",
        "                                        wd=weight_decay)\n",
        "    outputs = tf.nn.conv1d(inputs, kernel,\n",
        "                          stride=stride,\n",
        "                          padding=padding)\n",
        "    biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                              tf.constant_initializer(0.0))\n",
        "    outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "    if bn:\n",
        "      outputs = batch_norm_for_conv1d(outputs, is_training,\n",
        "                                      bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "    if activation_fn is not None:\n",
        "      outputs = activation_fn(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def conv2d(inputs,\n",
        "          num_output_channels,\n",
        "          kernel_size,\n",
        "          scope,\n",
        "          stride=[1, 1],\n",
        "          padding='SAME',\n",
        "          use_xavier=True,\n",
        "          stddev=1e-3,\n",
        "          weight_decay=0.0,\n",
        "          activation_fn=tf.nn.relu,\n",
        "          bn=False,\n",
        "          bn_decay=None,\n",
        "          is_training=None):\n",
        "  \"\"\" 2D convolution with non-linear operation.\n",
        "\n",
        "  Args:\n",
        "    inputs: 4-D tensor variable BxHxWxC\n",
        "    num_output_channels: int\n",
        "    kernel_size: a list of 2 ints\n",
        "    scope: string\n",
        "    stride: a list of 2 ints\n",
        "    padding: 'SAME' or 'VALID'\n",
        "    use_xavier: bool, use xavier_initializer if true\n",
        "    stddev: float, stddev for truncated_normal init\n",
        "    weight_decay: float\n",
        "    activation_fn: function\n",
        "    bn: bool, whether to use batch norm\n",
        "    bn_decay: float or float tensor variable in [0,1]\n",
        "    is_training: bool Tensor variable\n",
        "\n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  \n",
        "  with tf.variable_scope(scope) as sc:\n",
        "      kernel_h, kernel_w = kernel_size\n",
        "      num_in_channels = inputs.get_shape()[-1].value\n",
        "      kernel_shape = [kernel_h, kernel_w,\n",
        "                      num_in_channels, num_output_channels]\n",
        "      kernel = _variable_with_weight_decay('weights',\n",
        "                                          shape=kernel_shape,\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "      stride_h, stride_w = stride\n",
        "      outputs = tf.nn.conv2d(inputs, kernel,\n",
        "                            [1, stride_h, stride_w, 1],\n",
        "                            padding=padding)\n",
        "      biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "      if bn:\n",
        "        outputs = batch_norm_for_conv2d(outputs, is_training,\n",
        "                                        bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "def conv2d_transpose(inputs,\n",
        "                    num_output_channels,\n",
        "                    kernel_size,\n",
        "                    scope,\n",
        "                    stride=[1, 1],\n",
        "                    padding='SAME',\n",
        "                    use_xavier=True,\n",
        "                    stddev=1e-3,\n",
        "                    weight_decay=0.0,\n",
        "                    activation_fn=tf.nn.relu,\n",
        "                    bn=False,\n",
        "                    bn_decay=None,\n",
        "                    is_training=None):\n",
        "  \"\"\" 2D convolution transpose with non-linear operation.\n",
        "\n",
        "  Args:\n",
        "    inputs: 4-D tensor variable BxHxWxC\n",
        "    num_output_channels: int\n",
        "    kernel_size: a list of 2 ints\n",
        "    scope: string\n",
        "    stride: a list of 2 ints\n",
        "    padding: 'SAME' or 'VALID'\n",
        "    use_xavier: bool, use xavier_initializer if true\n",
        "    stddev: float, stddev for truncated_normal init\n",
        "    weight_decay: float\n",
        "    activation_fn: function\n",
        "    bn: bool, whether to use batch norm\n",
        "    bn_decay: float or float tensor variable in [0,1]\n",
        "    is_training: bool Tensor variable\n",
        "\n",
        "  Returns:\n",
        "    Variable tensor\n",
        "\n",
        "  Note: conv2d(conv2d_transpose(a, num_out, ksize, stride), a.shape[-1], ksize, stride) == a\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "      kernel_h, kernel_w = kernel_size\n",
        "      num_in_channels = inputs.get_shape()[-1].value\n",
        "      kernel_shape = [kernel_h, kernel_w,\n",
        "                      num_output_channels, num_in_channels] # reversed to conv2d\n",
        "      kernel = _variable_with_weight_decay('weights',\n",
        "                                          shape=kernel_shape,\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "      stride_h, stride_w = stride\n",
        "      \n",
        "      # from slim.convolution2d_transpose\n",
        "      def get_deconv_dim(dim_size, stride_size, kernel_size, padding):\n",
        "          dim_size *= stride_size\n",
        "\n",
        "          if padding == 'VALID' and dim_size is not None:\n",
        "            dim_size += max(kernel_size - stride_size, 0)\n",
        "          return dim_size\n",
        "\n",
        "      # caculate output shape\n",
        "      batch_size = inputs.get_shape()[0].value\n",
        "      height = inputs.get_shape()[1].value\n",
        "      width = inputs.get_shape()[2].value\n",
        "      out_height = get_deconv_dim(height, stride_h, kernel_h, padding)\n",
        "      out_width = get_deconv_dim(width, stride_w, kernel_w, padding)\n",
        "      output_shape = [batch_size, out_height, out_width, num_output_channels]\n",
        "\n",
        "      outputs = tf.nn.conv2d_transpose(inputs, kernel, output_shape,\n",
        "                            [1, stride_h, stride_w, 1],\n",
        "                            padding=padding)\n",
        "      biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "      if bn:\n",
        "        outputs = batch_norm_for_conv2d(outputs, is_training,\n",
        "                                        bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "  \n",
        "def conv3d(inputs,\n",
        "          num_output_channels,\n",
        "          kernel_size,\n",
        "          scope,\n",
        "          stride=[1, 1, 1],\n",
        "          padding='SAME',\n",
        "          use_xavier=True,\n",
        "          stddev=1e-3,\n",
        "          weight_decay=0.0,\n",
        "          activation_fn=tf.nn.relu,\n",
        "          bn=False,\n",
        "          bn_decay=None,\n",
        "          is_training=None):\n",
        "  \"\"\" 3D convolution with non-linear operation.\n",
        "\n",
        "  Args:\n",
        "    inputs: 5-D tensor variable BxDxHxWxC\n",
        "    num_output_channels: int\n",
        "    kernel_size: a list of 3 ints\n",
        "    scope: string\n",
        "    stride: a list of 3 ints\n",
        "    padding: 'SAME' or 'VALID'\n",
        "    use_xavier: bool, use xavier_initializer if true\n",
        "    stddev: float, stddev for truncated_normal init\n",
        "    weight_decay: float\n",
        "    activation_fn: function\n",
        "    bn: bool, whether to use batch norm\n",
        "    bn_decay: float or float tensor variable in [0,1]\n",
        "    is_training: bool Tensor variable\n",
        "\n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_d, kernel_h, kernel_w = kernel_size\n",
        "    num_in_channels = inputs.get_shape()[-1].value\n",
        "    kernel_shape = [kernel_d, kernel_h, kernel_w,\n",
        "                    num_in_channels, num_output_channels]\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                        shape=kernel_shape,\n",
        "                                        use_xavier=use_xavier,\n",
        "                                        stddev=stddev,\n",
        "                                        wd=weight_decay)\n",
        "    stride_d, stride_h, stride_w = stride\n",
        "    outputs = tf.nn.conv3d(inputs, kernel,\n",
        "                          [1, stride_d, stride_h, stride_w, 1],\n",
        "                          padding=padding)\n",
        "    biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                              tf.constant_initializer(0.0))\n",
        "    outputs = tf.nn.bias_add(outputs, biases)\n",
        "    \n",
        "    if bn:\n",
        "      outputs = batch_norm_for_conv3d(outputs, is_training,\n",
        "                                      bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "    if activation_fn is not None:\n",
        "      outputs = activation_fn(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def fully_connected(inputs,\n",
        "                    num_outputs,\n",
        "                    scope,\n",
        "                    use_xavier=True,\n",
        "                    stddev=1e-3,\n",
        "                    weight_decay=0.0,\n",
        "                    activation_fn=tf.nn.relu,\n",
        "                    bn=False,\n",
        "                    bn_decay=None,\n",
        "                    is_training=None):\n",
        "  \"\"\" Fully connected layer with non-linear operation.\n",
        "  \n",
        "  Args:\n",
        "    inputs: 2-D tensor BxN\n",
        "    num_outputs: int\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor of size B x num_outputs.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    num_input_units = inputs.get_shape()[-1].value\n",
        "    weights = _variable_with_weight_decay('weights',\n",
        "                                          shape=[num_input_units, num_outputs],\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "    outputs = tf.matmul(inputs, weights)\n",
        "    biases = _variable_on_cpu('biases', [num_outputs],\n",
        "                            tf.constant_initializer(0.0))\n",
        "    outputs = tf.nn.bias_add(outputs, biases)\n",
        "    \n",
        "    if bn:\n",
        "      outputs = batch_norm_for_fc(outputs, is_training, bn_decay, 'bn')\n",
        "\n",
        "    if activation_fn is not None:\n",
        "      outputs = activation_fn(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def max_pool2d(inputs,\n",
        "              kernel_size,\n",
        "              scope,\n",
        "              stride=[2, 2],\n",
        "              padding='VALID'):\n",
        "  \"\"\" 2D max pooling.\n",
        "\n",
        "  Args:\n",
        "    inputs: 4-D tensor BxHxWxC\n",
        "    kernel_size: a list of 2 ints\n",
        "    stride: a list of 2 ints\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_h, kernel_w = kernel_size\n",
        "    stride_h, stride_w = stride\n",
        "    outputs = tf.nn.max_pool(inputs,\n",
        "                            ksize=[1, kernel_h, kernel_w, 1],\n",
        "                            strides=[1, stride_h, stride_w, 1],\n",
        "                            padding=padding,\n",
        "                            name=sc.name)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def avg_pool2d(inputs,\n",
        "              kernel_size,\n",
        "              scope,\n",
        "              stride=[2, 2],\n",
        "              padding='VALID'):\n",
        "  \"\"\" 2D avg pooling.\n",
        "\n",
        "  Args:\n",
        "    inputs: 4-D tensor BxHxWxC\n",
        "    kernel_size: a list of 2 ints\n",
        "    stride: a list of 2 ints\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_h, kernel_w = kernel_size\n",
        "    stride_h, stride_w = stride\n",
        "    outputs = tf.nn.avg_pool(inputs,\n",
        "                            ksize=[1, kernel_h, kernel_w, 1],\n",
        "                            strides=[1, stride_h, stride_w, 1],\n",
        "                            padding=padding,\n",
        "                            name=sc.name)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def max_pool3d(inputs,\n",
        "              kernel_size,\n",
        "              scope,\n",
        "              stride=[2, 2, 2],\n",
        "              padding='VALID'):\n",
        "  \"\"\" 3D max pooling.\n",
        "\n",
        "  Args:\n",
        "    inputs: 5-D tensor BxDxHxWxC\n",
        "    kernel_size: a list of 3 ints\n",
        "    stride: a list of 3 ints\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_d, kernel_h, kernel_w = kernel_size\n",
        "    stride_d, stride_h, stride_w = stride\n",
        "    outputs = tf.nn.max_pool3d(inputs,\n",
        "                              ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
        "                              strides=[1, stride_d, stride_h, stride_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=sc.name)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def avg_pool3d(inputs,\n",
        "              kernel_size,\n",
        "              scope,\n",
        "              stride=[2, 2, 2],\n",
        "              padding='VALID'):\n",
        "  \"\"\" 3D avg pooling.\n",
        "\n",
        "  Args:\n",
        "    inputs: 5-D tensor BxDxHxWxC\n",
        "    kernel_size: a list of 3 ints\n",
        "    stride: a list of 3 ints\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_d, kernel_h, kernel_w = kernel_size\n",
        "    stride_d, stride_h, stride_w = stride\n",
        "    outputs = tf.nn.avg_pool3d(inputs,\n",
        "                              ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
        "                              strides=[1, stride_d, stride_h, stride_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=sc.name)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def batch_norm_template(inputs, is_training, scope, moments_dims, bn_decay):\n",
        "  \"\"\" Batch normalization on convolutional maps and beyond...\n",
        "  Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
        "  \n",
        "  Args:\n",
        "      inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n",
        "      is_training:   boolean tf.Varialbe, true indicates training phase\n",
        "      scope:         string, variable scope\n",
        "      moments_dims:  a list of ints, indicating dimensions for moments calculation\n",
        "      bn_decay:      float or float tensor variable, controling moving average weight\n",
        "  Return:\n",
        "      normed:        batch-normalized maps\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    num_channels = inputs.get_shape()[-1].value\n",
        "    beta = tf.Variable(tf.constant(0.0, shape=[num_channels]),\n",
        "                      name='beta', trainable=True)\n",
        "    gamma = tf.Variable(tf.constant(1.0, shape=[num_channels]),\n",
        "                        name='gamma', trainable=True)\n",
        "    batch_mean, batch_var = tf.nn.moments(inputs, moments_dims, name='moments')\n",
        "    decay = bn_decay if bn_decay is not None else 0.9\n",
        "    ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
        "    # Operator that maintains moving averages of variables.\n",
        "    ema_apply_op = tf.cond(is_training,\n",
        "                          lambda: ema.apply([batch_mean, batch_var]),\n",
        "                          lambda: tf.no_op())\n",
        "    \n",
        "    # Update moving average and return current batch's avg and var.\n",
        "    def mean_var_with_update():\n",
        "      with tf.control_dependencies([ema_apply_op]):\n",
        "        return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "    \n",
        "    # ema.average returns the Variable holding the average of var.\n",
        "    mean, var = tf.cond(is_training,\n",
        "                        mean_var_with_update,\n",
        "                        lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "    normed = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, 1e-3)\n",
        "  return normed\n",
        "\n",
        "\n",
        "def batch_norm_for_fc(inputs, is_training, bn_decay, scope):\n",
        "  \"\"\" Batch normalization on FC data.\n",
        "  \n",
        "  Args:\n",
        "      inputs:      Tensor, 2D BxC input\n",
        "      is_training: boolean tf.Varialbe, true indicates training phase\n",
        "      bn_decay:    float or float tensor variable, controling moving average weight\n",
        "      scope:       string, variable scope\n",
        "  Return:\n",
        "      normed:      batch-normalized maps\n",
        "  \"\"\"\n",
        "  return batch_norm_template(inputs, is_training, scope, [0,], bn_decay)\n",
        "\n",
        "\n",
        "def batch_norm_for_conv1d(inputs, is_training, bn_decay, scope):\n",
        "  \"\"\" Batch normalization on 1D convolutional maps.\n",
        "  \n",
        "  Args:\n",
        "      inputs:      Tensor, 3D BLC input maps\n",
        "      is_training: boolean tf.Varialbe, true indicates training phase\n",
        "      bn_decay:    float or float tensor variable, controling moving average weight\n",
        "      scope:       string, variable scope\n",
        "  Return:\n",
        "      normed:      batch-normalized maps\n",
        "  \"\"\"\n",
        "  return batch_norm_template(inputs, is_training, scope, [0,1], bn_decay)\n",
        "\n",
        "\n",
        "def batch_norm_for_conv2d(inputs, is_training, bn_decay, scope):\n",
        "  \"\"\" Batch normalization on 2D convolutional maps.\n",
        "  \n",
        "  Args:\n",
        "      inputs:      Tensor, 4D BHWC input maps\n",
        "      is_training: boolean tf.Varialbe, true indicates training phase\n",
        "      bn_decay:    float or float tensor variable, controling moving average weight\n",
        "      scope:       string, variable scope\n",
        "  Return:\n",
        "      normed:      batch-normalized maps\n",
        "  \"\"\"\n",
        "  return batch_norm_template(inputs, is_training, scope, [0,1,2], bn_decay)\n",
        "\n",
        "\n",
        "def batch_norm_for_conv3d(inputs, is_training, bn_decay, scope):\n",
        "  \"\"\" Batch normalization on 3D convolutional maps.\n",
        "  \n",
        "  Args:\n",
        "      inputs:      Tensor, 5D BDHWC input maps\n",
        "      is_training: boolean tf.Varialbe, true indicates training phase\n",
        "      bn_decay:    float or float tensor variable, controling moving average weight\n",
        "      scope:       string, variable scope\n",
        "  Return:\n",
        "      normed:      batch-normalized maps\n",
        "  \"\"\"\n",
        "  return batch_norm_template(inputs, is_training, scope, [0,1,2,3], bn_decay)\n",
        "\n",
        "\n",
        "def dropout(inputs,\n",
        "            is_training,\n",
        "            scope,\n",
        "            keep_prob=0.5,\n",
        "            noise_shape=None):\n",
        "  \"\"\" Dropout layer.\n",
        "\n",
        "  Args:\n",
        "    inputs: tensor\n",
        "    is_training: boolean tf.Variable\n",
        "    scope: string\n",
        "    keep_prob: float in [0,1]\n",
        "    noise_shape: list of ints\n",
        "\n",
        "  Returns:\n",
        "    tensor variable\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    outputs = tf.cond(is_training,\n",
        "                      lambda: tf.nn.dropout(inputs, keep_prob, noise_shape),\n",
        "                      lambda: inputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "class TF_util():\n",
        "  @staticmethod\n",
        "  def _variable_on_cpu(name, shape, initializer, use_fp16=False):\n",
        "    \"\"\"Helper to create a Variable stored on CPU memory.\n",
        "    Args:\n",
        "      name: name of the variable\n",
        "      shape: list of ints\n",
        "      initializer: initializer for Variable\n",
        "    Returns:\n",
        "      Variable Tensor\n",
        "    \"\"\"\n",
        "    with tf.device('/cpu:0'):\n",
        "      dtype = tf.float16 if use_fp16 else tf.float32\n",
        "      var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
        "    return var\n",
        "\n",
        "  @staticmethod\n",
        "  def _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):\n",
        "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
        "\n",
        "    Note that the Variable is initialized with a truncated normal distribution.\n",
        "    A weight decay is added only if one is specified.\n",
        "\n",
        "    Args:\n",
        "      name: name of the variable\n",
        "      shape: list of ints\n",
        "      stddev: standard deviation of a truncated Gaussian\n",
        "      wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "          decay is not added for this Variable.\n",
        "      use_xavier: bool, whether to use xavier initializer\n",
        "\n",
        "    Returns:\n",
        "      Variable Tensor\n",
        "    \"\"\"\n",
        "    if use_xavier:\n",
        "      initializer = tf.contrib.layers.xavier_initializer()\n",
        "    else:\n",
        "      initializer = tf.truncated_normal_initializer(stddev=stddev)\n",
        "    var = _variable_on_cpu(name, shape, initializer)\n",
        "    if wd is not None:\n",
        "      weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "      tf.add_to_collection('losses', weight_decay)\n",
        "    return var\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def conv1d(inputs,\n",
        "            num_output_channels,\n",
        "            kernel_size,\n",
        "            scope,\n",
        "            stride=1,\n",
        "            padding='SAME',\n",
        "            use_xavier=True,\n",
        "            stddev=1e-3,\n",
        "            weight_decay=0.0,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            bn=False,\n",
        "            bn_decay=None,\n",
        "            is_training=None):\n",
        "    \"\"\" 1D convolution with non-linear operation.\n",
        "\n",
        "    Args:\n",
        "      inputs: 3-D tensor variable BxLxC\n",
        "      num_output_channels: int\n",
        "      kernel_size: int\n",
        "      scope: string\n",
        "      stride: int\n",
        "      padding: 'SAME' or 'VALID'\n",
        "      use_xavier: bool, use xavier_initializer if true\n",
        "      stddev: float, stddev for truncated_normal init\n",
        "      weight_decay: float\n",
        "      activation_fn: function\n",
        "      bn: bool, whether to use batch norm\n",
        "      bn_decay: float or float tensor variable in [0,1]\n",
        "      is_training: bool Tensor variable\n",
        "\n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      num_in_channels = inputs.get_shape()[-1].value\n",
        "      kernel_shape = [kernel_size,\n",
        "                      num_in_channels, num_output_channels]\n",
        "      kernel = _variable_with_weight_decay('weights',\n",
        "                                          shape=kernel_shape,\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "      outputs = tf.nn.conv1d(inputs, kernel,\n",
        "                            stride=stride,\n",
        "                            padding=padding)\n",
        "      biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "      if bn:\n",
        "        outputs = batch_norm_for_conv1d(outputs, is_training,\n",
        "                                        bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def conv2d(inputs,\n",
        "            num_output_channels,\n",
        "            kernel_size,\n",
        "            scope,\n",
        "            stride=[1, 1],\n",
        "            padding='SAME',\n",
        "            use_xavier=True,\n",
        "            stddev=1e-3,\n",
        "            weight_decay=0.0,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            bn=False,\n",
        "            bn_decay=None,\n",
        "            is_training=None):\n",
        "    \"\"\" 2D convolution with non-linear operation.\n",
        "\n",
        "    Args:\n",
        "      inputs: 4-D tensor variable BxHxWxC\n",
        "      num_output_channels: int\n",
        "      kernel_size: a list of 2 ints\n",
        "      scope: string\n",
        "      stride: a list of 2 ints\n",
        "      padding: 'SAME' or 'VALID'\n",
        "      use_xavier: bool, use xavier_initializer if true\n",
        "      stddev: float, stddev for truncated_normal init\n",
        "      weight_decay: float\n",
        "      activation_fn: function\n",
        "      bn: bool, whether to use batch norm\n",
        "      bn_decay: float or float tensor variable in [0,1]\n",
        "      is_training: bool Tensor variable\n",
        "\n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    \n",
        "    with tf.variable_scope(scope) as sc:\n",
        "        kernel_h, kernel_w = kernel_size\n",
        "        num_in_channels = inputs.get_shape()[-1].value\n",
        "        kernel_shape = [kernel_h, kernel_w,\n",
        "                        num_in_channels, num_output_channels]\n",
        "        kernel = _variable_with_weight_decay('weights',\n",
        "                                            shape=kernel_shape,\n",
        "                                            use_xavier=use_xavier,\n",
        "                                            stddev=stddev,\n",
        "                                            wd=weight_decay)\n",
        "        stride_h, stride_w = stride\n",
        "        outputs = tf.nn.conv2d(inputs, kernel,\n",
        "                              [1, stride_h, stride_w, 1],\n",
        "                              padding=padding)\n",
        "        biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                  tf.constant_initializer(0.0))\n",
        "        outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "        if bn:\n",
        "          outputs = batch_norm_for_conv2d(outputs, is_training,\n",
        "                                          bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "        if activation_fn is not None:\n",
        "          outputs = activation_fn(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def conv2d_transpose(inputs,\n",
        "                      num_output_channels,\n",
        "                      kernel_size,\n",
        "                      scope,\n",
        "                      stride=[1, 1],\n",
        "                      padding='SAME',\n",
        "                      use_xavier=True,\n",
        "                      stddev=1e-3,\n",
        "                      weight_decay=0.0,\n",
        "                      activation_fn=tf.nn.relu,\n",
        "                      bn=False,\n",
        "                      bn_decay=None,\n",
        "                      is_training=None):\n",
        "    \"\"\" 2D convolution transpose with non-linear operation.\n",
        "\n",
        "    Args:\n",
        "      inputs: 4-D tensor variable BxHxWxC\n",
        "      num_output_channels: int\n",
        "      kernel_size: a list of 2 ints\n",
        "      scope: string\n",
        "      stride: a list of 2 ints\n",
        "      padding: 'SAME' or 'VALID'\n",
        "      use_xavier: bool, use xavier_initializer if true\n",
        "      stddev: float, stddev for truncated_normal init\n",
        "      weight_decay: float\n",
        "      activation_fn: function\n",
        "      bn: bool, whether to use batch norm\n",
        "      bn_decay: float or float tensor variable in [0,1]\n",
        "      is_training: bool Tensor variable\n",
        "\n",
        "    Returns:\n",
        "      Variable tensor\n",
        "\n",
        "    Note: conv2d(conv2d_transpose(a, num_out, ksize, stride), a.shape[-1], ksize, stride) == a\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "        kernel_h, kernel_w = kernel_size\n",
        "        num_in_channels = inputs.get_shape()[-1].value\n",
        "        kernel_shape = [kernel_h, kernel_w,\n",
        "                        num_output_channels, num_in_channels] # reversed to conv2d\n",
        "        kernel = _variable_with_weight_decay('weights',\n",
        "                                            shape=kernel_shape,\n",
        "                                            use_xavier=use_xavier,\n",
        "                                            stddev=stddev,\n",
        "                                            wd=weight_decay)\n",
        "        stride_h, stride_w = stride\n",
        "        \n",
        "        # from slim.convolution2d_transpose\n",
        "        def get_deconv_dim(dim_size, stride_size, kernel_size, padding):\n",
        "            dim_size *= stride_size\n",
        "\n",
        "            if padding == 'VALID' and dim_size is not None:\n",
        "              dim_size += max(kernel_size - stride_size, 0)\n",
        "            return dim_size\n",
        "\n",
        "        # caculate output shape\n",
        "        batch_size = inputs.get_shape()[0].value\n",
        "        height = inputs.get_shape()[1].value\n",
        "        width = inputs.get_shape()[2].value\n",
        "        out_height = get_deconv_dim(height, stride_h, kernel_h, padding)\n",
        "        out_width = get_deconv_dim(width, stride_w, kernel_w, padding)\n",
        "        output_shape = [batch_size, out_height, out_width, num_output_channels]\n",
        "\n",
        "        outputs = tf.nn.conv2d_transpose(inputs, kernel, output_shape,\n",
        "                              [1, stride_h, stride_w, 1],\n",
        "                              padding=padding)\n",
        "        biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                  tf.constant_initializer(0.0))\n",
        "        outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "        if bn:\n",
        "          outputs = batch_norm_for_conv2d(outputs, is_training,\n",
        "                                          bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "        if activation_fn is not None:\n",
        "          outputs = activation_fn(outputs)\n",
        "        return outputs\n",
        "\n",
        "    \n",
        "  @staticmethod\n",
        "  def conv3d(inputs,\n",
        "            num_output_channels,\n",
        "            kernel_size,\n",
        "            scope,\n",
        "            stride=[1, 1, 1],\n",
        "            padding='SAME',\n",
        "            use_xavier=True,\n",
        "            stddev=1e-3,\n",
        "            weight_decay=0.0,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            bn=False,\n",
        "            bn_decay=None,\n",
        "            is_training=None):\n",
        "    \"\"\" 3D convolution with non-linear operation.\n",
        "\n",
        "    Args:\n",
        "      inputs: 5-D tensor variable BxDxHxWxC\n",
        "      num_output_channels: int\n",
        "      kernel_size: a list of 3 ints\n",
        "      scope: string\n",
        "      stride: a list of 3 ints\n",
        "      padding: 'SAME' or 'VALID'\n",
        "      use_xavier: bool, use xavier_initializer if true\n",
        "      stddev: float, stddev for truncated_normal init\n",
        "      weight_decay: float\n",
        "      activation_fn: function\n",
        "      bn: bool, whether to use batch norm\n",
        "      bn_decay: float or float tensor variable in [0,1]\n",
        "      is_training: bool Tensor variable\n",
        "\n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_d, kernel_h, kernel_w = kernel_size\n",
        "      num_in_channels = inputs.get_shape()[-1].value\n",
        "      kernel_shape = [kernel_d, kernel_h, kernel_w,\n",
        "                      num_in_channels, num_output_channels]\n",
        "      kernel = _variable_with_weight_decay('weights',\n",
        "                                          shape=kernel_shape,\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "      stride_d, stride_h, stride_w = stride\n",
        "      outputs = tf.nn.conv3d(inputs, kernel,\n",
        "                            [1, stride_d, stride_h, stride_w, 1],\n",
        "                            padding=padding)\n",
        "      biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "      \n",
        "      if bn:\n",
        "        outputs = batch_norm_for_conv3d(outputs, is_training,\n",
        "                                        bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def fully_connected(inputs,\n",
        "                      num_outputs,\n",
        "                      scope,\n",
        "                      use_xavier=True,\n",
        "                      stddev=1e-3,\n",
        "                      weight_decay=0.0,\n",
        "                      activation_fn=tf.nn.relu,\n",
        "                      bn=False,\n",
        "                      bn_decay=None,\n",
        "                      is_training=None):\n",
        "    \"\"\" Fully connected layer with non-linear operation.\n",
        "    \n",
        "    Args:\n",
        "      inputs: 2-D tensor BxN\n",
        "      num_outputs: int\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor of size B x num_outputs.\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      num_input_units = inputs.get_shape()[-1].value\n",
        "      weights = _variable_with_weight_decay('weights',\n",
        "                                            shape=[num_input_units, num_outputs],\n",
        "                                            use_xavier=use_xavier,\n",
        "                                            stddev=stddev,\n",
        "                                            wd=weight_decay)\n",
        "      outputs = tf.matmul(inputs, weights)\n",
        "      biases = _variable_on_cpu('biases', [num_outputs],\n",
        "                              tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "      \n",
        "      if bn:\n",
        "        outputs = batch_norm_for_fc(outputs, is_training, bn_decay, 'bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "  @staticmethod\n",
        "  def max_pool2d(inputs,\n",
        "                kernel_size,\n",
        "                scope,\n",
        "                stride=[2, 2],\n",
        "                padding='VALID'):\n",
        "    \"\"\" 2D max pooling.\n",
        "\n",
        "    Args:\n",
        "      inputs: 4-D tensor BxHxWxC\n",
        "      kernel_size: a list of 2 ints\n",
        "      stride: a list of 2 ints\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_h, kernel_w = kernel_size\n",
        "      stride_h, stride_w = stride\n",
        "      outputs = tf.nn.max_pool(inputs,\n",
        "                              ksize=[1, kernel_h, kernel_w, 1],\n",
        "                              strides=[1, stride_h, stride_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=sc.name)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def avg_pool2d(inputs,\n",
        "                kernel_size,\n",
        "                scope,\n",
        "                stride=[2, 2],\n",
        "                padding='VALID'):\n",
        "    \"\"\" 2D avg pooling.\n",
        "\n",
        "    Args:\n",
        "      inputs: 4-D tensor BxHxWxC\n",
        "      kernel_size: a list of 2 ints\n",
        "      stride: a list of 2 ints\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_h, kernel_w = kernel_size\n",
        "      stride_h, stride_w = stride\n",
        "      outputs = tf.nn.avg_pool(inputs,\n",
        "                              ksize=[1, kernel_h, kernel_w, 1],\n",
        "                              strides=[1, stride_h, stride_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=sc.name)\n",
        "      return outputs\n",
        "\n",
        "  @staticmethod\n",
        "  def max_pool3d(inputs,\n",
        "                kernel_size,\n",
        "                scope,\n",
        "                stride=[2, 2, 2],\n",
        "                padding='VALID'):\n",
        "    \"\"\" 3D max pooling.\n",
        "\n",
        "    Args:\n",
        "      inputs: 5-D tensor BxDxHxWxC\n",
        "      kernel_size: a list of 3 ints\n",
        "      stride: a list of 3 ints\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_d, kernel_h, kernel_w = kernel_size\n",
        "      stride_d, stride_h, stride_w = stride\n",
        "      outputs = tf.nn.max_pool3d(inputs,\n",
        "                                ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
        "                                strides=[1, stride_d, stride_h, stride_w, 1],\n",
        "                                padding=padding,\n",
        "                                name=sc.name)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def avg_pool3d(inputs,\n",
        "                kernel_size,\n",
        "                scope,\n",
        "                stride=[2, 2, 2],\n",
        "                padding='VALID'):\n",
        "    \"\"\" 3D avg pooling.\n",
        "\n",
        "    Args:\n",
        "      inputs: 5-D tensor BxDxHxWxC\n",
        "      kernel_size: a list of 3 ints\n",
        "      stride: a list of 3 ints\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_d, kernel_h, kernel_w = kernel_size\n",
        "      stride_d, stride_h, stride_w = stride\n",
        "      outputs = tf.nn.avg_pool3d(inputs,\n",
        "                                ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
        "                                strides=[1, stride_d, stride_h, stride_w, 1],\n",
        "                                padding=padding,\n",
        "                                name=sc.name)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_template(inputs, is_training, scope, moments_dims, bn_decay):\n",
        "    \"\"\" Batch normalization on convolutional maps and beyond...\n",
        "    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
        "    \n",
        "    Args:\n",
        "        inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n",
        "        is_training:   boolean tf.Varialbe, true indicates training phase\n",
        "        scope:         string, variable scope\n",
        "        moments_dims:  a list of ints, indicating dimensions for moments calculation\n",
        "        bn_decay:      float or float tensor variable, controling moving average weight\n",
        "    Return:\n",
        "        normed:        batch-normalized maps\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      num_channels = inputs.get_shape()[-1].value\n",
        "      beta = tf.Variable(tf.constant(0.0, shape=[num_channels]),\n",
        "                        name='beta', trainable=True)\n",
        "      gamma = tf.Variable(tf.constant(1.0, shape=[num_channels]),\n",
        "                          name='gamma', trainable=True)\n",
        "      batch_mean, batch_var = tf.nn.moments(inputs, moments_dims, name='moments')\n",
        "      decay = bn_decay if bn_decay is not None else 0.9\n",
        "      ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
        "      # Operator that maintains moving averages of variables.\n",
        "      ema_apply_op = tf.cond(is_training,\n",
        "                            lambda: ema.apply([batch_mean, batch_var]),\n",
        "                            lambda: tf.no_op())\n",
        "      \n",
        "      # Update moving average and return current batch's avg and var.\n",
        "      def mean_var_with_update():\n",
        "        with tf.control_dependencies([ema_apply_op]):\n",
        "          return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "      \n",
        "      # ema.average returns the Variable holding the average of var.\n",
        "      mean, var = tf.cond(is_training,\n",
        "                          mean_var_with_update,\n",
        "                          lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "      normed = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, 1e-3)\n",
        "    return normed\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_for_fc(inputs, is_training, bn_decay, scope):\n",
        "    \"\"\" Batch normalization on FC data.\n",
        "    \n",
        "    Args:\n",
        "        inputs:      Tensor, 2D BxC input\n",
        "        is_training: boolean tf.Varialbe, true indicates training phase\n",
        "        bn_decay:    float or float tensor variable, controling moving average weight\n",
        "        scope:       string, variable scope\n",
        "    Return:\n",
        "        normed:      batch-normalized maps\n",
        "    \"\"\"\n",
        "    return batch_norm_template(inputs, is_training, scope, [0,], bn_decay)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_for_conv1d(inputs, is_training, bn_decay, scope):\n",
        "    \"\"\" Batch normalization on 1D convolutional maps.\n",
        "    \n",
        "    Args:\n",
        "        inputs:      Tensor, 3D BLC input maps\n",
        "        is_training: boolean tf.Varialbe, true indicates training phase\n",
        "        bn_decay:    float or float tensor variable, controling moving average weight\n",
        "        scope:       string, variable scope\n",
        "    Return:\n",
        "        normed:      batch-normalized maps\n",
        "    \"\"\"\n",
        "    return batch_norm_template(inputs, is_training, scope, [0,1], bn_decay)\n",
        "\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_for_conv2d(inputs, is_training, bn_decay, scope):\n",
        "    \"\"\" Batch normalization on 2D convolutional maps.\n",
        "    \n",
        "    Args:\n",
        "        inputs:      Tensor, 4D BHWC input maps\n",
        "        is_training: boolean tf.Varialbe, true indicates training phase\n",
        "        bn_decay:    float or float tensor variable, controling moving average weight\n",
        "        scope:       string, variable scope\n",
        "    Return:\n",
        "        normed:      batch-normalized maps\n",
        "    \"\"\"\n",
        "    return batch_norm_template(inputs, is_training, scope, [0,1,2], bn_decay)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_for_conv3d(inputs, is_training, bn_decay, scope):\n",
        "    \"\"\" Batch normalization on 3D convolutional maps.\n",
        "    \n",
        "    Args:\n",
        "        inputs:      Tensor, 5D BDHWC input maps\n",
        "        is_training: boolean tf.Varialbe, true indicates training phase\n",
        "        bn_decay:    float or float tensor variable, controling moving average weight\n",
        "        scope:       string, variable scope\n",
        "    Return:\n",
        "        normed:      batch-normalized maps\n",
        "    \"\"\"\n",
        "    return batch_norm_template(inputs, is_training, scope, [0,1,2,3], bn_decay)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def dropout(inputs,\n",
        "              is_training,\n",
        "              scope,\n",
        "              keep_prob=0.5,\n",
        "              noise_shape=None):\n",
        "    \"\"\" Dropout layer.\n",
        "\n",
        "    Args:\n",
        "      inputs: tensor\n",
        "      is_training: boolean tf.Variable\n",
        "      scope: string\n",
        "      keep_prob: float in [0,1]\n",
        "      noise_shape: list of ints\n",
        "\n",
        "    Returns:\n",
        "      tensor variable\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      outputs = tf.cond(is_training,\n",
        "                        lambda: tf.nn.dropout(inputs, keep_prob, noise_shape),\n",
        "                        lambda: inputs)\n",
        "      return outputs\n",
        "\n",
        "tf_util = TF_util()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVLyuktonKYg"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy8thGIn8oWZ"
      },
      "source": [
        "# Global constants:\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_POINT = 1024\n",
        "MAX_EPOCH = 250\n",
        "BASE_LEARNING_RATE = 0.001\n",
        "GPU_INDEX = 0\n",
        "MOMENTUM = 0.9\n",
        "OPTIMIZER = 'adam'\n",
        "DECAY_STEP = 20000\n",
        "DECAY_RATE = 0.8\n",
        "\n",
        "MAX_NUM_POINT = 2048\n",
        "NUN_CLASSES = 40\n",
        "\n",
        "BN_INIT_DECAY = 0.5\n",
        "BN_DECAY_DECAY_RATE = 0.5\n",
        "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
        "BN_DECAY_CLIP = 0.99"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGU1LDoAw7Fl"
      },
      "source": [
        "def even_sample(mesh, n, r):\n",
        "    samples = np.zeros((1, 1))\n",
        "    while samples.shape[0] != n:\n",
        "        samples = sample_surface_even(mesh, n, r)[0]\n",
        "        r /= 10\n",
        "    return samples"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnP3yfcaiWDu"
      },
      "source": [
        "dir_path = '/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/ModelNet10/ModelNet10/'\n",
        "classes = os.listdir(dir_path)\n",
        "train_labels, test_labels = [], []\n",
        "train_data, test_data = [], []\n",
        "train_faces, test_faces = [], []\n",
        "\n",
        "for c in tqdm(classes):\n",
        "\n",
        "  # Train data loading:\n",
        "  for mesh in tqdm(os.listdir(dir_path + c + '/train')):\n",
        "    train_labels += [c]\n",
        "    train_mesh = trimesh.load_mesh(dir_path + c + '/train/' + mesh)\n",
        "    train_data += [np.array(even_sample(train_mesh, NUM_POINT, 1))]\n",
        "\n",
        "  # Test data loading:\n",
        "  for mesh in tqdm(os.listdir(dir_path + c + '/test')):\n",
        "    test_labels += [c]\n",
        "    test_mesh = trimesh.load_mesh(dir_path + c + '/test/' + mesh)\n",
        "    test_data += [np.array(even_sample(test_mesh, NUM_POINT, 1))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-qd5ylV4kcK"
      },
      "source": [
        "train_data_concatenated = np.concatenate([a[np.newaxis, :, :] for a in train_data], axis=0)\n",
        "test_data_concatenated = np.concatenate([a[np.newaxis, :, :] for a in test_data], axis=0)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0x8Fki19bXv"
      },
      "source": [
        "with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/train_data.npy', 'wb') as train_file:\n",
        "  np.save(train_file, train_data_concatenated)\n",
        "\n",
        "with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/test_data.npy', 'wb') as test_file:\n",
        "  np.save(test_file, test_data_concatenated)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OshcE3Waj1Uv"
      },
      "source": [
        "# Run basic experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pD2lhD2iWdU"
      },
      "source": [
        "## Classic **PointNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovXx0aUTihnX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73IeJRoBiiLF"
      },
      "source": [
        "## Classic **Momenet**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp7jCqFHxQo3"
      },
      "source": [
        "class Momenet():\n",
        "  def placeholder_inputs(batch_size, num_point):\n",
        "      pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 9))\n",
        "      labels_pl = tf.placeholder(tf.int32, shape=(batch_size))\n",
        "      return pointclouds_pl, labels_pl\n",
        "\n",
        "\n",
        "  def get_model(point_cloud, is_training, bn_decay=None):\n",
        "      \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
        "      batch_size = point_cloud.get_shape()[0].value\n",
        "      num_point = point_cloud.get_shape()[1].value\n",
        "      end_points = {}\n",
        "      input_image = tf.expand_dims(point_cloud, -1)\n",
        "      net = 1\n",
        "      # Point functions (MLP implemented as conv2d)\n",
        "      net = tf_util.conv2d(input_image, 64, [1,9], 'conv1',\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          bn_decay=bn_decay)\n",
        "      net = tf_util.conv2d(net, 64, [1,1],\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          scope='conv2', bn_decay=bn_decay)\n",
        "      net = tf_util.conv2d(net, 64, [1,1],\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          scope='conv3', bn_decay=bn_decay)\n",
        "      net = tf_util.conv2d(net, 128, [1,1],\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          scope='conv4', bn_decay=bn_decay)\n",
        "      net = tf_util.conv2d(net, 1024, [1,1],\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          scope='conv5', bn_decay=bn_decay)\n",
        "\n",
        "      # Symmetric function: max pooling\n",
        "      net = tf_util.max_pool2d(net, [num_point,1],\n",
        "                              padding='VALID', scope='maxpool')\n",
        "      \n",
        "      # MLP on global point cloud vector\n",
        "      net = tf.reshape(net, [batch_size, -1])\n",
        "      net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
        "                                    scope='fc1', bn_decay=bn_decay)\n",
        "      net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
        "                                    scope='fc2', bn_decay=bn_decay)\n",
        "      net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n",
        "                            scope='dp1')\n",
        "      net = tf_util.fully_connected(net, 40, activation_fn=None, scope='fc3')\n",
        "\n",
        "      return net, end_points\n",
        "\n",
        "\n",
        "  def get_loss(pred, label, end_points):\n",
        "      \"\"\" pred: B*NUM_CLASSES,\n",
        "          label: B, \"\"\"\n",
        "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
        "      classify_loss = tf.reduce_mean(loss)\n",
        "      tf.summary.scalar('classify loss', classify_loss)\n",
        "      return classify_loss\n",
        "\n",
        "MODEL = Momenet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_FarEV8Q_9S"
      },
      "source": [
        "LOG_DIR = '/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/Momenet/log'\n",
        "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpHKMaN06cAc",
        "outputId": "f2bdfc29-6eb7-40e1-d33a-3fbde671aa75"
      },
      "source": [
        "# Sanity check that get_model works\n",
        "with tf.Graph().as_default():\n",
        "  inputs = tf.zeros((32, 1024, 9))\n",
        "  outputs = MODEL.get_model(inputs, tf.constant(True))\n",
        "  print(outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor 'fc3/BiasAdd:0' shape=(32, 40) dtype=float32>, {})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "259kAz4Sep-c"
      },
      "source": [
        "def add_moments(data, order=2):\n",
        "  if order == 1:\n",
        "    return data\n",
        "  elif order == 2:\n",
        "    data_moments = np.zeros((np.shape(data)[0], np.shape(data)[1], 9))\n",
        "  elif order == 3:\n",
        "    raise ValueError('3rd order moments are not prepared yet!')\n",
        "  \n",
        "  data_moments[:, :, 0:3] = data\n",
        "  data_moments[:, :, 3] = data_moments[:, :, 0] * data_moments[:, :, 0]\n",
        "  data_moments[:, :, 4] = data_moments[:, :, 1] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 5] = data_moments[:, :, 2] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 6] = data_moments[:, :, 0] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 7] = data_moments[:, :, 0] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 8] = data_moments[:, :, 1] * data_moments[:, :, 2]\n",
        "\n",
        "  return data_moments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mIEwc6GUFYt"
      },
      "source": [
        "def get_bn_decay(batch):\n",
        "  bn_momentum = tf.train.exponential_decay(\n",
        "                    BN_INIT_DECAY,\n",
        "                    batch*BATCH_SIZE,\n",
        "                    BN_DECAY_DECAY_STEP,\n",
        "                    BN_DECAY_DECAY_RATE,\n",
        "                    staircase=True)\n",
        "  bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
        "  return bn_decay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hraDbagYpi3r"
      },
      "source": [
        "def log_string(out_str):\n",
        "    LOG_FOUT.write(out_str+'\\n')\n",
        "    LOG_FOUT.flush()\n",
        "    print(out_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub-GfCTxn1FV"
      },
      "source": [
        "def get_learning_rate(batch):\n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
        "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
        "                        DECAY_STEP,          # Decay step.\n",
        "                        DECAY_RATE,          # Decay rate.\n",
        "                        staircase=True)\n",
        "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
        "    return learning_rate   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX9gACkTqa_z"
      },
      "source": [
        "def train_one_epoch(sess, ops, train_writer):\n",
        "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
        "    is_training = True\n",
        "    \n",
        "    # Shuffle train files\n",
        "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
        "    np.random.shuffle(train_file_idxs)\n",
        "    \n",
        "    # for fn in range(len(TRAIN_FILES)):\n",
        "    #     log_string('----' + str(fn) + '-----')\n",
        "    #     current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]])\n",
        "    #     current_data = current_data[:,0:NUM_POINT,:]\n",
        "    #     current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))            \n",
        "    #     current_label = np.squeeze(current_label)\n",
        "        \n",
        "        \n",
        "        \n",
        "    #     file_size = current_data.shape[0]\n",
        "    #     num_batches = file_size // BATCH_SIZE\n",
        "        \n",
        "    #     total_correct = 0\n",
        "    #     total_seen = 0\n",
        "    #     loss_sum = 0\n",
        "       \n",
        "    #     for batch_idx in range(num_batches):\n",
        "    #         start_idx = batch_idx * BATCH_SIZE\n",
        "    #         end_idx = (batch_idx+1) * BATCH_SIZE\n",
        "\n",
        "    #         # Augment batched point clouds by rotation and jittering\n",
        "    #         rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
        "    #         jittered_data = provider.jitter_point_cloud(rotated_data)\n",
        "    #         feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
        "    #                      ops['labels_pl']: current_label[start_idx:end_idx],\n",
        "    #                      ops['is_training_pl']: is_training,}\n",
        "            \n",
        "           \n",
        "            \n",
        "    #         summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
        "    #             ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
        "            \n",
        "            \n",
        "    #         train_writer.add_summary(summary, step)\n",
        "    #         pred_val = np.argmax(pred_val, 1)\n",
        "    #         correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
        "    #         total_correct += correct\n",
        "    #         total_seen += BATCH_SIZE\n",
        "    #         loss_sum += loss_val\n",
        "        \n",
        "    #     log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
        "    #     log_string('accuracy: %f' % (total_correct / float(total_seen)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc-PQcI9i5tU"
      },
      "source": [
        "def train():\n",
        "  with tf.Graph().as_default():\n",
        "    with tf.device('/gpu:' + str(GPU_INDEX)):\n",
        "      pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
        "      is_training_pl = tf.placeholder(tf.bool, shape=())\n",
        "      print(is_training_pl)\n",
        "      \n",
        "      # Note the global_step=batch parameter to minimize. \n",
        "      # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
        "      batch = tf.Variable(0)\n",
        "      bn_decay = get_bn_decay(batch)\n",
        "      tf.summary.scalar('bn_decay', bn_decay)\n",
        "\n",
        "      # Get model and loss \n",
        "      pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
        "      loss = MODEL.get_loss(pred, labels_pl, end_points)\n",
        "      tf.summary.scalar('loss', loss)\n",
        "\n",
        "      correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n",
        "      accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
        "      tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "      # Get training operator\n",
        "      learning_rate = get_learning_rate(batch)\n",
        "      tf.summary.scalar('learning_rate', learning_rate)\n",
        "      if OPTIMIZER == 'momentum':\n",
        "          optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
        "      elif OPTIMIZER == 'adam':\n",
        "          optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "      train_op = optimizer.minimize(loss, global_step=batch)\n",
        "      \n",
        "      # Add ops to save and restore all the variables.\n",
        "      saver = tf.train.Saver()\n",
        "          \n",
        "      # Create a session\n",
        "      config = tf.ConfigProto()\n",
        "      config.gpu_options.allow_growth = True\n",
        "      config.allow_soft_placement = True\n",
        "      config.log_device_placement = False\n",
        "      sess = tf.Session(config=config)\n",
        "\n",
        "      # Add summary writers\n",
        "      #merged = tf.merge_all_summaries()\n",
        "      merged = tf.summary.merge_all()\n",
        "      train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
        "                                sess.graph)\n",
        "      test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
        "\n",
        "      # Init variables\n",
        "      init = tf.global_variables_initializer()\n",
        "      # To fix the bug introduced in TF 0.12.1 as in\n",
        "      # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
        "      #sess.run(init)\n",
        "      sess.run(init, {is_training_pl: True})\n",
        "\n",
        "      ops = {'pointclouds_pl': pointclouds_pl,\n",
        "              'labels_pl': labels_pl,\n",
        "              'is_training_pl': is_training_pl,\n",
        "              'pred': pred,\n",
        "              'loss': loss,\n",
        "              'train_op': train_op,\n",
        "              'merged': merged,\n",
        "              'step': batch}\n",
        "      best_acc=0\n",
        "      for epoch in range(MAX_EPOCH):\n",
        "          log_string('**** EPOCH %03d ****' % (epoch))\n",
        "          # sys.stdout.flush()\n",
        "            \n",
        "          train_one_epoch(sess, ops, train_writer)\n",
        "          # acc = eval_one_epoch(sess, ops, test_writer)\n",
        "          \n",
        "          # if acc > best_acc:\n",
        "          #     save_path = saver.save(sess, os.path.join(LOG_DIR, \"best_model.ckpt\"))\n",
        "          #     log_string(\"Best Model saved in file: %s\" % save_path)\n",
        "          #     best_acc = acc\n",
        "          \n",
        "          # # Save the variables to disk.\n",
        "          # if epoch % 10 == 0:\n",
        "          #     save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
        "          #     log_string(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78UDJvSvi6Pe"
      },
      "source": [
        "## $1^{st}$, $2^{nd}$ and $3^{rd}$ order moments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m9TMUU-jbWe"
      },
      "source": [
        "def add_moments(data, order=2):\n",
        "  if order == 1:\n",
        "    return data\n",
        "  elif order == 2:\n",
        "    data_moments = np.zeros((np.shape(data)[0], np.shape(data)[1], 9))\n",
        "  elif order == 3:\n",
        "    raise ValueError('3rd order moments are not prepared yet!')\n",
        "  \n",
        "  data_moments[:, :, 0:3] = data\n",
        "  data_moments[:, :, 3] = data_moments[:, :, 0] * data_moments[:, :, 0]\n",
        "  data_moments[:, :, 4] = data_moments[:, :, 1] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 5] = data_moments[:, :, 2] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 6] = data_moments[:, :, 0] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 7] = data_moments[:, :, 0] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 8] = data_moments[:, :, 1] * data_moments[:, :, 2]\n",
        "\n",
        "  \n",
        "\n",
        "  return data_moments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLmjOJ2CjckY"
      },
      "source": [
        "# Add consistently oriented vertex normals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96T6Qm4BjzeB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsldZFbTj-0r"
      },
      "source": [
        "## Classic **PointNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbt66gNTkGgD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq9R0Jj3kG63"
      },
      "source": [
        "## Classic **Momenet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irBRCORskJbI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGKDTjNIkJ1S"
      },
      "source": [
        "## $1^{st}$, $2^{nd}$ and $3^{rd}$ order moments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNRNny7OkMXP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOzdFxEjkQN4"
      },
      "source": [
        "# Add another geometric prelifting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X48dGnaOkTlS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sr9SPlRkW15"
      },
      "source": [
        "## Basic runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUrPwBSCkZNp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoWfndBXkZz9"
      },
      "source": [
        "## Consistently oriented vertex normals runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfME_OKF33LR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}