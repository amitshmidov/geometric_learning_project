{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geometric_learning_final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5_a7zJA-iDrw",
        "WoI2KClxM6d9"
      ],
      "authorship_tag": "ABX9TyNdfcxIBTZksaonjlDoSYEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6dd8bb313e014a3fbb3e0e7b5ccf8679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9c6fd36f004e4edd948499842f81d5ca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_823fb77ab266468fad38b8ca28478d95",
              "IPY_MODEL_0a0e8de7e97843e2aa18c69e3da67643"
            ]
          }
        },
        "9c6fd36f004e4edd948499842f81d5ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "823fb77ab266468fad38b8ca28478d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0e5be25907914cc9922a4dc40774ba02",
            "_dom_classes": [],
            "description": "  3%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e57002b2fd25444fb6af5d2fa4be9ef0"
          }
        },
        "0a0e8de7e97843e2aa18c69e3da67643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_19915736695540858d9eb2706c878817",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7/250 [01:23&lt;48:19, 11.93s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae6288870c874c329c527632eefcfef4"
          }
        },
        "0e5be25907914cc9922a4dc40774ba02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e57002b2fd25444fb6af5d2fa4be9ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19915736695540858d9eb2706c878817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae6288870c874c329c527632eefcfef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitshmidov/geometric_learning_project/blob/main/geometric_learning_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_a7zJA-iDrw"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0REbDcKoc_3"
      },
      "source": [
        "!pip install pyvista\n",
        "!pip install trimesh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xChOBbx4nStU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7799102d-c84a-4895-c5bc-4af9c564bf0f"
      },
      "source": [
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import pyvista as pv\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import trimesh\n",
        "from tqdm.notebook import tqdm\n",
        "from trimesh.sample import sample_surface_even\n",
        "\n",
        "from typing import Tuple\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoI2KClxM6d9"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spcm39HPNAZC"
      },
      "source": [
        "def _variable_on_cpu(name, shape, initializer, use_fp16=False):\n",
        "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
        "  Args:\n",
        "    name: name of the variable\n",
        "    shape: list of ints\n",
        "    initializer: initializer for Variable\n",
        "  Returns:\n",
        "    Variable Tensor\n",
        "  \"\"\"\n",
        "  with tf.device('/cpu:0'):\n",
        "    dtype = tf.float16 if use_fp16 else tf.float32\n",
        "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
        "  return var\n",
        "\n",
        "\n",
        "def _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):\n",
        "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
        "\n",
        "  Note that the Variable is initialized with a truncated normal distribution.\n",
        "  A weight decay is added only if one is specified.\n",
        "\n",
        "  Args:\n",
        "    name: name of the variable\n",
        "    shape: list of ints\n",
        "    stddev: standard deviation of a truncated Gaussian\n",
        "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "        decay is not added for this Variable.\n",
        "    use_xavier: bool, whether to use xavier initializer\n",
        "\n",
        "  Returns:\n",
        "    Variable Tensor\n",
        "  \"\"\"\n",
        "  if use_xavier:\n",
        "    initializer = tf.contrib.layers.xavier_initializer()\n",
        "  else:\n",
        "    initializer = tf.truncated_normal_initializer(stddev=stddev)\n",
        "  var = _variable_on_cpu(name, shape, initializer)\n",
        "  if wd is not None:\n",
        "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "    tf.add_to_collection('losses', weight_decay)\n",
        "  return var\n",
        "\n",
        "\n",
        "def conv1d(inputs,\n",
        "          num_output_channels,\n",
        "          kernel_size,\n",
        "          scope,\n",
        "          stride=1,\n",
        "          padding='SAME',\n",
        "          use_xavier=True,\n",
        "          stddev=1e-3,\n",
        "          weight_decay=0.0,\n",
        "          activation_fn=tf.nn.relu,\n",
        "          bn=False,\n",
        "          bn_decay=None,\n",
        "          is_training=None):\n",
        "  \"\"\" 1D convolution with non-linear operation.\n",
        "\n",
        "  Args:\n",
        "    inputs: 3-D tensor variable BxLxC\n",
        "    num_output_channels: int\n",
        "    kernel_size: int\n",
        "    scope: string\n",
        "    stride: int\n",
        "    padding: 'SAME' or 'VALID'\n",
        "    use_xavier: bool, use xavier_initializer if true\n",
        "    stddev: float, stddev for truncated_normal init\n",
        "    weight_decay: float\n",
        "    activation_fn: function\n",
        "    bn: bool, whether to use batch norm\n",
        "    bn_decay: float or float tensor variable in [0,1]\n",
        "    is_training: bool Tensor variable\n",
        "\n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    num_in_channels = inputs.get_shape()[-1].value\n",
        "    kernel_shape = [kernel_size,\n",
        "                    num_in_channels, num_output_channels]\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                        shape=kernel_shape,\n",
        "                                        use_xavier=use_xavier,\n",
        "                                        stddev=stddev,\n",
        "                                        wd=weight_decay)\n",
        "    outputs = tf.nn.conv1d(inputs, kernel,\n",
        "                          stride=stride,\n",
        "                          padding=padding)\n",
        "    biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                              tf.constant_initializer(0.0))\n",
        "    outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "    if bn:\n",
        "      outputs = batch_norm_for_conv1d(outputs, is_training,\n",
        "                                      bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "    if activation_fn is not None:\n",
        "      outputs = activation_fn(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def conv2d(inputs,\n",
        "          num_output_channels,\n",
        "          kernel_size,\n",
        "          scope,\n",
        "          stride=[1, 1],\n",
        "          padding='SAME',\n",
        "          use_xavier=True,\n",
        "          stddev=1e-3,\n",
        "          weight_decay=0.0,\n",
        "          activation_fn=tf.nn.relu,\n",
        "          bn=False,\n",
        "          bn_decay=None,\n",
        "          is_training=None):\n",
        "  \"\"\" 2D convolution with non-linear operation.\n",
        "\n",
        "  Args:\n",
        "    inputs: 4-D tensor variable BxHxWxC\n",
        "    num_output_channels: int\n",
        "    kernel_size: a list of 2 ints\n",
        "    scope: string\n",
        "    stride: a list of 2 ints\n",
        "    padding: 'SAME' or 'VALID'\n",
        "    use_xavier: bool, use xavier_initializer if true\n",
        "    stddev: float, stddev for truncated_normal init\n",
        "    weight_decay: float\n",
        "    activation_fn: function\n",
        "    bn: bool, whether to use batch norm\n",
        "    bn_decay: float or float tensor variable in [0,1]\n",
        "    is_training: bool Tensor variable\n",
        "\n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  \n",
        "  with tf.variable_scope(scope) as sc:\n",
        "      kernel_h, kernel_w = kernel_size\n",
        "      num_in_channels = inputs.get_shape()[-1].value\n",
        "      kernel_shape = [kernel_h, kernel_w,\n",
        "                      num_in_channels, num_output_channels]\n",
        "      kernel = _variable_with_weight_decay('weights',\n",
        "                                          shape=kernel_shape,\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "      stride_h, stride_w = stride\n",
        "      outputs = tf.nn.conv2d(inputs, kernel,\n",
        "                            [1, stride_h, stride_w, 1],\n",
        "                            padding=padding)\n",
        "      biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "      if bn:\n",
        "        outputs = batch_norm_for_conv2d(outputs, is_training,\n",
        "                                        bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "def conv2d_transpose(inputs,\n",
        "                    num_output_channels,\n",
        "                    kernel_size,\n",
        "                    scope,\n",
        "                    stride=[1, 1],\n",
        "                    padding='SAME',\n",
        "                    use_xavier=True,\n",
        "                    stddev=1e-3,\n",
        "                    weight_decay=0.0,\n",
        "                    activation_fn=tf.nn.relu,\n",
        "                    bn=False,\n",
        "                    bn_decay=None,\n",
        "                    is_training=None):\n",
        "  \"\"\" 2D convolution transpose with non-linear operation.\n",
        "\n",
        "  Args:\n",
        "    inputs: 4-D tensor variable BxHxWxC\n",
        "    num_output_channels: int\n",
        "    kernel_size: a list of 2 ints\n",
        "    scope: string\n",
        "    stride: a list of 2 ints\n",
        "    padding: 'SAME' or 'VALID'\n",
        "    use_xavier: bool, use xavier_initializer if true\n",
        "    stddev: float, stddev for truncated_normal init\n",
        "    weight_decay: float\n",
        "    activation_fn: function\n",
        "    bn: bool, whether to use batch norm\n",
        "    bn_decay: float or float tensor variable in [0,1]\n",
        "    is_training: bool Tensor variable\n",
        "\n",
        "  Returns:\n",
        "    Variable tensor\n",
        "\n",
        "  Note: conv2d(conv2d_transpose(a, num_out, ksize, stride), a.shape[-1], ksize, stride) == a\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "      kernel_h, kernel_w = kernel_size\n",
        "      num_in_channels = inputs.get_shape()[-1].value\n",
        "      kernel_shape = [kernel_h, kernel_w,\n",
        "                      num_output_channels, num_in_channels] # reversed to conv2d\n",
        "      kernel = _variable_with_weight_decay('weights',\n",
        "                                          shape=kernel_shape,\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "      stride_h, stride_w = stride\n",
        "      \n",
        "      # from slim.convolution2d_transpose\n",
        "      def get_deconv_dim(dim_size, stride_size, kernel_size, padding):\n",
        "          dim_size *= stride_size\n",
        "\n",
        "          if padding == 'VALID' and dim_size is not None:\n",
        "            dim_size += max(kernel_size - stride_size, 0)\n",
        "          return dim_size\n",
        "\n",
        "      # caculate output shape\n",
        "      batch_size = inputs.get_shape()[0].value\n",
        "      height = inputs.get_shape()[1].value\n",
        "      width = inputs.get_shape()[2].value\n",
        "      out_height = get_deconv_dim(height, stride_h, kernel_h, padding)\n",
        "      out_width = get_deconv_dim(width, stride_w, kernel_w, padding)\n",
        "      output_shape = [batch_size, out_height, out_width, num_output_channels]\n",
        "\n",
        "      outputs = tf.nn.conv2d_transpose(inputs, kernel, output_shape,\n",
        "                            [1, stride_h, stride_w, 1],\n",
        "                            padding=padding)\n",
        "      biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "      if bn:\n",
        "        outputs = batch_norm_for_conv2d(outputs, is_training,\n",
        "                                        bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "  \n",
        "def conv3d(inputs,\n",
        "          num_output_channels,\n",
        "          kernel_size,\n",
        "          scope,\n",
        "          stride=[1, 1, 1],\n",
        "          padding='SAME',\n",
        "          use_xavier=True,\n",
        "          stddev=1e-3,\n",
        "          weight_decay=0.0,\n",
        "          activation_fn=tf.nn.relu,\n",
        "          bn=False,\n",
        "          bn_decay=None,\n",
        "          is_training=None):\n",
        "  \"\"\" 3D convolution with non-linear operation.\n",
        "\n",
        "  Args:\n",
        "    inputs: 5-D tensor variable BxDxHxWxC\n",
        "    num_output_channels: int\n",
        "    kernel_size: a list of 3 ints\n",
        "    scope: string\n",
        "    stride: a list of 3 ints\n",
        "    padding: 'SAME' or 'VALID'\n",
        "    use_xavier: bool, use xavier_initializer if true\n",
        "    stddev: float, stddev for truncated_normal init\n",
        "    weight_decay: float\n",
        "    activation_fn: function\n",
        "    bn: bool, whether to use batch norm\n",
        "    bn_decay: float or float tensor variable in [0,1]\n",
        "    is_training: bool Tensor variable\n",
        "\n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_d, kernel_h, kernel_w = kernel_size\n",
        "    num_in_channels = inputs.get_shape()[-1].value\n",
        "    kernel_shape = [kernel_d, kernel_h, kernel_w,\n",
        "                    num_in_channels, num_output_channels]\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                        shape=kernel_shape,\n",
        "                                        use_xavier=use_xavier,\n",
        "                                        stddev=stddev,\n",
        "                                        wd=weight_decay)\n",
        "    stride_d, stride_h, stride_w = stride\n",
        "    outputs = tf.nn.conv3d(inputs, kernel,\n",
        "                          [1, stride_d, stride_h, stride_w, 1],\n",
        "                          padding=padding)\n",
        "    biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                              tf.constant_initializer(0.0))\n",
        "    outputs = tf.nn.bias_add(outputs, biases)\n",
        "    \n",
        "    if bn:\n",
        "      outputs = batch_norm_for_conv3d(outputs, is_training,\n",
        "                                      bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "    if activation_fn is not None:\n",
        "      outputs = activation_fn(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def fully_connected(inputs,\n",
        "                    num_outputs,\n",
        "                    scope,\n",
        "                    use_xavier=True,\n",
        "                    stddev=1e-3,\n",
        "                    weight_decay=0.0,\n",
        "                    activation_fn=tf.nn.relu,\n",
        "                    bn=False,\n",
        "                    bn_decay=None,\n",
        "                    is_training=None):\n",
        "  \"\"\" Fully connected layer with non-linear operation.\n",
        "  \n",
        "  Args:\n",
        "    inputs: 2-D tensor BxN\n",
        "    num_outputs: int\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor of size B x num_outputs.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    num_input_units = inputs.get_shape()[-1].value\n",
        "    weights = _variable_with_weight_decay('weights',\n",
        "                                          shape=[num_input_units, num_outputs],\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "    outputs = tf.matmul(inputs, weights)\n",
        "    biases = _variable_on_cpu('biases', [num_outputs],\n",
        "                            tf.constant_initializer(0.0))\n",
        "    outputs = tf.nn.bias_add(outputs, biases)\n",
        "    \n",
        "    if bn:\n",
        "      outputs = batch_norm_for_fc(outputs, is_training, bn_decay, 'bn')\n",
        "\n",
        "    if activation_fn is not None:\n",
        "      outputs = activation_fn(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def max_pool2d(inputs,\n",
        "              kernel_size,\n",
        "              scope,\n",
        "              stride=[2, 2],\n",
        "              padding='VALID'):\n",
        "  \"\"\" 2D max pooling.\n",
        "\n",
        "  Args:\n",
        "    inputs: 4-D tensor BxHxWxC\n",
        "    kernel_size: a list of 2 ints\n",
        "    stride: a list of 2 ints\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_h, kernel_w = kernel_size\n",
        "    stride_h, stride_w = stride\n",
        "    outputs = tf.nn.max_pool(inputs,\n",
        "                            ksize=[1, kernel_h, kernel_w, 1],\n",
        "                            strides=[1, stride_h, stride_w, 1],\n",
        "                            padding=padding,\n",
        "                            name=sc.name)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def avg_pool2d(inputs,\n",
        "              kernel_size,\n",
        "              scope,\n",
        "              stride=[2, 2],\n",
        "              padding='VALID'):\n",
        "  \"\"\" 2D avg pooling.\n",
        "\n",
        "  Args:\n",
        "    inputs: 4-D tensor BxHxWxC\n",
        "    kernel_size: a list of 2 ints\n",
        "    stride: a list of 2 ints\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_h, kernel_w = kernel_size\n",
        "    stride_h, stride_w = stride\n",
        "    outputs = tf.nn.avg_pool(inputs,\n",
        "                            ksize=[1, kernel_h, kernel_w, 1],\n",
        "                            strides=[1, stride_h, stride_w, 1],\n",
        "                            padding=padding,\n",
        "                            name=sc.name)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def max_pool3d(inputs,\n",
        "              kernel_size,\n",
        "              scope,\n",
        "              stride=[2, 2, 2],\n",
        "              padding='VALID'):\n",
        "  \"\"\" 3D max pooling.\n",
        "\n",
        "  Args:\n",
        "    inputs: 5-D tensor BxDxHxWxC\n",
        "    kernel_size: a list of 3 ints\n",
        "    stride: a list of 3 ints\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_d, kernel_h, kernel_w = kernel_size\n",
        "    stride_d, stride_h, stride_w = stride\n",
        "    outputs = tf.nn.max_pool3d(inputs,\n",
        "                              ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
        "                              strides=[1, stride_d, stride_h, stride_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=sc.name)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def avg_pool3d(inputs,\n",
        "              kernel_size,\n",
        "              scope,\n",
        "              stride=[2, 2, 2],\n",
        "              padding='VALID'):\n",
        "  \"\"\" 3D avg pooling.\n",
        "\n",
        "  Args:\n",
        "    inputs: 5-D tensor BxDxHxWxC\n",
        "    kernel_size: a list of 3 ints\n",
        "    stride: a list of 3 ints\n",
        "  \n",
        "  Returns:\n",
        "    Variable tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    kernel_d, kernel_h, kernel_w = kernel_size\n",
        "    stride_d, stride_h, stride_w = stride\n",
        "    outputs = tf.nn.avg_pool3d(inputs,\n",
        "                              ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
        "                              strides=[1, stride_d, stride_h, stride_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=sc.name)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def batch_norm_template(inputs, is_training, scope, moments_dims, bn_decay):\n",
        "  \"\"\" Batch normalization on convolutional maps and beyond...\n",
        "  Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
        "  \n",
        "  Args:\n",
        "      inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n",
        "      is_training:   boolean tf.Varialbe, true indicates training phase\n",
        "      scope:         string, variable scope\n",
        "      moments_dims:  a list of ints, indicating dimensions for moments calculation\n",
        "      bn_decay:      float or float tensor variable, controling moving average weight\n",
        "  Return:\n",
        "      normed:        batch-normalized maps\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    num_channels = inputs.get_shape()[-1].value\n",
        "    beta = tf.Variable(tf.constant(0.0, shape=[num_channels]),\n",
        "                      name='beta', trainable=True)\n",
        "    gamma = tf.Variable(tf.constant(1.0, shape=[num_channels]),\n",
        "                        name='gamma', trainable=True)\n",
        "    batch_mean, batch_var = tf.nn.moments(inputs, moments_dims, name='moments')\n",
        "    decay = bn_decay if bn_decay is not None else 0.9\n",
        "    ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
        "    # Operator that maintains moving averages of variables.\n",
        "    ema_apply_op = tf.cond(is_training,\n",
        "                          lambda: ema.apply([batch_mean, batch_var]),\n",
        "                          lambda: tf.no_op())\n",
        "    \n",
        "    # Update moving average and return current batch's avg and var.\n",
        "    def mean_var_with_update():\n",
        "      with tf.control_dependencies([ema_apply_op]):\n",
        "        return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "    \n",
        "    # ema.average returns the Variable holding the average of var.\n",
        "    mean, var = tf.cond(is_training,\n",
        "                        mean_var_with_update,\n",
        "                        lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "    normed = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, 1e-3)\n",
        "  return normed\n",
        "\n",
        "\n",
        "def batch_norm_for_fc(inputs, is_training, bn_decay, scope):\n",
        "  \"\"\" Batch normalization on FC data.\n",
        "  \n",
        "  Args:\n",
        "      inputs:      Tensor, 2D BxC input\n",
        "      is_training: boolean tf.Varialbe, true indicates training phase\n",
        "      bn_decay:    float or float tensor variable, controling moving average weight\n",
        "      scope:       string, variable scope\n",
        "  Return:\n",
        "      normed:      batch-normalized maps\n",
        "  \"\"\"\n",
        "  return batch_norm_template(inputs, is_training, scope, [0,], bn_decay)\n",
        "\n",
        "\n",
        "def batch_norm_for_conv1d(inputs, is_training, bn_decay, scope):\n",
        "  \"\"\" Batch normalization on 1D convolutional maps.\n",
        "  \n",
        "  Args:\n",
        "      inputs:      Tensor, 3D BLC input maps\n",
        "      is_training: boolean tf.Varialbe, true indicates training phase\n",
        "      bn_decay:    float or float tensor variable, controling moving average weight\n",
        "      scope:       string, variable scope\n",
        "  Return:\n",
        "      normed:      batch-normalized maps\n",
        "  \"\"\"\n",
        "  return batch_norm_template(inputs, is_training, scope, [0,1], bn_decay)\n",
        "\n",
        "\n",
        "def batch_norm_for_conv2d(inputs, is_training, bn_decay, scope):\n",
        "  \"\"\" Batch normalization on 2D convolutional maps.\n",
        "  \n",
        "  Args:\n",
        "      inputs:      Tensor, 4D BHWC input maps\n",
        "      is_training: boolean tf.Varialbe, true indicates training phase\n",
        "      bn_decay:    float or float tensor variable, controling moving average weight\n",
        "      scope:       string, variable scope\n",
        "  Return:\n",
        "      normed:      batch-normalized maps\n",
        "  \"\"\"\n",
        "  return batch_norm_template(inputs, is_training, scope, [0,1,2], bn_decay)\n",
        "\n",
        "\n",
        "def batch_norm_for_conv3d(inputs, is_training, bn_decay, scope):\n",
        "  \"\"\" Batch normalization on 3D convolutional maps.\n",
        "  \n",
        "  Args:\n",
        "      inputs:      Tensor, 5D BDHWC input maps\n",
        "      is_training: boolean tf.Varialbe, true indicates training phase\n",
        "      bn_decay:    float or float tensor variable, controling moving average weight\n",
        "      scope:       string, variable scope\n",
        "  Return:\n",
        "      normed:      batch-normalized maps\n",
        "  \"\"\"\n",
        "  return batch_norm_template(inputs, is_training, scope, [0,1,2,3], bn_decay)\n",
        "\n",
        "\n",
        "def dropout(inputs,\n",
        "            is_training,\n",
        "            scope,\n",
        "            keep_prob=0.5,\n",
        "            noise_shape=None):\n",
        "  \"\"\" Dropout layer.\n",
        "\n",
        "  Args:\n",
        "    inputs: tensor\n",
        "    is_training: boolean tf.Variable\n",
        "    scope: string\n",
        "    keep_prob: float in [0,1]\n",
        "    noise_shape: list of ints\n",
        "\n",
        "  Returns:\n",
        "    tensor variable\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope) as sc:\n",
        "    outputs = tf.cond(is_training,\n",
        "                      lambda: tf.nn.dropout(inputs, keep_prob, noise_shape),\n",
        "                      lambda: inputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "class TF_util():\n",
        "  @staticmethod\n",
        "  def _variable_on_cpu(name, shape, initializer, use_fp16=False):\n",
        "    \"\"\"Helper to create a Variable stored on CPU memory.\n",
        "    Args:\n",
        "      name: name of the variable\n",
        "      shape: list of ints\n",
        "      initializer: initializer for Variable\n",
        "    Returns:\n",
        "      Variable Tensor\n",
        "    \"\"\"\n",
        "    with tf.device('/cpu:0'):\n",
        "      dtype = tf.float16 if use_fp16 else tf.float32\n",
        "      var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
        "    return var\n",
        "\n",
        "  @staticmethod\n",
        "  def _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):\n",
        "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
        "\n",
        "    Note that the Variable is initialized with a truncated normal distribution.\n",
        "    A weight decay is added only if one is specified.\n",
        "\n",
        "    Args:\n",
        "      name: name of the variable\n",
        "      shape: list of ints\n",
        "      stddev: standard deviation of a truncated Gaussian\n",
        "      wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "          decay is not added for this Variable.\n",
        "      use_xavier: bool, whether to use xavier initializer\n",
        "\n",
        "    Returns:\n",
        "      Variable Tensor\n",
        "    \"\"\"\n",
        "    if use_xavier:\n",
        "      initializer = tf.contrib.layers.xavier_initializer()\n",
        "    else:\n",
        "      initializer = tf.truncated_normal_initializer(stddev=stddev)\n",
        "    var = _variable_on_cpu(name, shape, initializer)\n",
        "    if wd is not None:\n",
        "      weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "      tf.add_to_collection('losses', weight_decay)\n",
        "    return var\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def conv1d(inputs,\n",
        "            num_output_channels,\n",
        "            kernel_size,\n",
        "            scope,\n",
        "            stride=1,\n",
        "            padding='SAME',\n",
        "            use_xavier=True,\n",
        "            stddev=1e-3,\n",
        "            weight_decay=0.0,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            bn=False,\n",
        "            bn_decay=None,\n",
        "            is_training=None):\n",
        "    \"\"\" 1D convolution with non-linear operation.\n",
        "\n",
        "    Args:\n",
        "      inputs: 3-D tensor variable BxLxC\n",
        "      num_output_channels: int\n",
        "      kernel_size: int\n",
        "      scope: string\n",
        "      stride: int\n",
        "      padding: 'SAME' or 'VALID'\n",
        "      use_xavier: bool, use xavier_initializer if true\n",
        "      stddev: float, stddev for truncated_normal init\n",
        "      weight_decay: float\n",
        "      activation_fn: function\n",
        "      bn: bool, whether to use batch norm\n",
        "      bn_decay: float or float tensor variable in [0,1]\n",
        "      is_training: bool Tensor variable\n",
        "\n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      num_in_channels = inputs.get_shape()[-1].value\n",
        "      kernel_shape = [kernel_size,\n",
        "                      num_in_channels, num_output_channels]\n",
        "      kernel = _variable_with_weight_decay('weights',\n",
        "                                          shape=kernel_shape,\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "      outputs = tf.nn.conv1d(inputs, kernel,\n",
        "                            stride=stride,\n",
        "                            padding=padding)\n",
        "      biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "      if bn:\n",
        "        outputs = batch_norm_for_conv1d(outputs, is_training,\n",
        "                                        bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def conv2d(inputs,\n",
        "            num_output_channels,\n",
        "            kernel_size,\n",
        "            scope,\n",
        "            stride=[1, 1],\n",
        "            padding='SAME',\n",
        "            use_xavier=True,\n",
        "            stddev=1e-3,\n",
        "            weight_decay=0.0,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            bn=False,\n",
        "            bn_decay=None,\n",
        "            is_training=None):\n",
        "    \"\"\" 2D convolution with non-linear operation.\n",
        "\n",
        "    Args:\n",
        "      inputs: 4-D tensor variable BxHxWxC\n",
        "      num_output_channels: int\n",
        "      kernel_size: a list of 2 ints\n",
        "      scope: string\n",
        "      stride: a list of 2 ints\n",
        "      padding: 'SAME' or 'VALID'\n",
        "      use_xavier: bool, use xavier_initializer if true\n",
        "      stddev: float, stddev for truncated_normal init\n",
        "      weight_decay: float\n",
        "      activation_fn: function\n",
        "      bn: bool, whether to use batch norm\n",
        "      bn_decay: float or float tensor variable in [0,1]\n",
        "      is_training: bool Tensor variable\n",
        "\n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    \n",
        "    with tf.variable_scope(scope) as sc:\n",
        "        kernel_h, kernel_w = kernel_size\n",
        "        num_in_channels = inputs.get_shape()[-1].value\n",
        "        kernel_shape = [kernel_h, kernel_w,\n",
        "                        num_in_channels, num_output_channels]\n",
        "        kernel = _variable_with_weight_decay('weights',\n",
        "                                            shape=kernel_shape,\n",
        "                                            use_xavier=use_xavier,\n",
        "                                            stddev=stddev,\n",
        "                                            wd=weight_decay)\n",
        "        stride_h, stride_w = stride\n",
        "        outputs = tf.nn.conv2d(inputs, kernel,\n",
        "                              [1, stride_h, stride_w, 1],\n",
        "                              padding=padding)\n",
        "        biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                  tf.constant_initializer(0.0))\n",
        "        outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "        if bn:\n",
        "          outputs = batch_norm_for_conv2d(outputs, is_training,\n",
        "                                          bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "        if activation_fn is not None:\n",
        "          outputs = activation_fn(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def conv2d_transpose(inputs,\n",
        "                      num_output_channels,\n",
        "                      kernel_size,\n",
        "                      scope,\n",
        "                      stride=[1, 1],\n",
        "                      padding='SAME',\n",
        "                      use_xavier=True,\n",
        "                      stddev=1e-3,\n",
        "                      weight_decay=0.0,\n",
        "                      activation_fn=tf.nn.relu,\n",
        "                      bn=False,\n",
        "                      bn_decay=None,\n",
        "                      is_training=None):\n",
        "    \"\"\" 2D convolution transpose with non-linear operation.\n",
        "\n",
        "    Args:\n",
        "      inputs: 4-D tensor variable BxHxWxC\n",
        "      num_output_channels: int\n",
        "      kernel_size: a list of 2 ints\n",
        "      scope: string\n",
        "      stride: a list of 2 ints\n",
        "      padding: 'SAME' or 'VALID'\n",
        "      use_xavier: bool, use xavier_initializer if true\n",
        "      stddev: float, stddev for truncated_normal init\n",
        "      weight_decay: float\n",
        "      activation_fn: function\n",
        "      bn: bool, whether to use batch norm\n",
        "      bn_decay: float or float tensor variable in [0,1]\n",
        "      is_training: bool Tensor variable\n",
        "\n",
        "    Returns:\n",
        "      Variable tensor\n",
        "\n",
        "    Note: conv2d(conv2d_transpose(a, num_out, ksize, stride), a.shape[-1], ksize, stride) == a\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "        kernel_h, kernel_w = kernel_size\n",
        "        num_in_channels = inputs.get_shape()[-1].value\n",
        "        kernel_shape = [kernel_h, kernel_w,\n",
        "                        num_output_channels, num_in_channels] # reversed to conv2d\n",
        "        kernel = _variable_with_weight_decay('weights',\n",
        "                                            shape=kernel_shape,\n",
        "                                            use_xavier=use_xavier,\n",
        "                                            stddev=stddev,\n",
        "                                            wd=weight_decay)\n",
        "        stride_h, stride_w = stride\n",
        "        \n",
        "        # from slim.convolution2d_transpose\n",
        "        def get_deconv_dim(dim_size, stride_size, kernel_size, padding):\n",
        "            dim_size *= stride_size\n",
        "\n",
        "            if padding == 'VALID' and dim_size is not None:\n",
        "              dim_size += max(kernel_size - stride_size, 0)\n",
        "            return dim_size\n",
        "\n",
        "        # caculate output shape\n",
        "        batch_size = inputs.get_shape()[0].value\n",
        "        height = inputs.get_shape()[1].value\n",
        "        width = inputs.get_shape()[2].value\n",
        "        out_height = get_deconv_dim(height, stride_h, kernel_h, padding)\n",
        "        out_width = get_deconv_dim(width, stride_w, kernel_w, padding)\n",
        "        output_shape = [batch_size, out_height, out_width, num_output_channels]\n",
        "\n",
        "        outputs = tf.nn.conv2d_transpose(inputs, kernel, output_shape,\n",
        "                              [1, stride_h, stride_w, 1],\n",
        "                              padding=padding)\n",
        "        biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                  tf.constant_initializer(0.0))\n",
        "        outputs = tf.nn.bias_add(outputs, biases)\n",
        "\n",
        "        if bn:\n",
        "          outputs = batch_norm_for_conv2d(outputs, is_training,\n",
        "                                          bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "        if activation_fn is not None:\n",
        "          outputs = activation_fn(outputs)\n",
        "        return outputs\n",
        "\n",
        "    \n",
        "  @staticmethod\n",
        "  def conv3d(inputs,\n",
        "            num_output_channels,\n",
        "            kernel_size,\n",
        "            scope,\n",
        "            stride=[1, 1, 1],\n",
        "            padding='SAME',\n",
        "            use_xavier=True,\n",
        "            stddev=1e-3,\n",
        "            weight_decay=0.0,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            bn=False,\n",
        "            bn_decay=None,\n",
        "            is_training=None):\n",
        "    \"\"\" 3D convolution with non-linear operation.\n",
        "\n",
        "    Args:\n",
        "      inputs: 5-D tensor variable BxDxHxWxC\n",
        "      num_output_channels: int\n",
        "      kernel_size: a list of 3 ints\n",
        "      scope: string\n",
        "      stride: a list of 3 ints\n",
        "      padding: 'SAME' or 'VALID'\n",
        "      use_xavier: bool, use xavier_initializer if true\n",
        "      stddev: float, stddev for truncated_normal init\n",
        "      weight_decay: float\n",
        "      activation_fn: function\n",
        "      bn: bool, whether to use batch norm\n",
        "      bn_decay: float or float tensor variable in [0,1]\n",
        "      is_training: bool Tensor variable\n",
        "\n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_d, kernel_h, kernel_w = kernel_size\n",
        "      num_in_channels = inputs.get_shape()[-1].value\n",
        "      kernel_shape = [kernel_d, kernel_h, kernel_w,\n",
        "                      num_in_channels, num_output_channels]\n",
        "      kernel = _variable_with_weight_decay('weights',\n",
        "                                          shape=kernel_shape,\n",
        "                                          use_xavier=use_xavier,\n",
        "                                          stddev=stddev,\n",
        "                                          wd=weight_decay)\n",
        "      stride_d, stride_h, stride_w = stride\n",
        "      outputs = tf.nn.conv3d(inputs, kernel,\n",
        "                            [1, stride_d, stride_h, stride_w, 1],\n",
        "                            padding=padding)\n",
        "      biases = _variable_on_cpu('biases', [num_output_channels],\n",
        "                                tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "      \n",
        "      if bn:\n",
        "        outputs = batch_norm_for_conv3d(outputs, is_training,\n",
        "                                        bn_decay=bn_decay, scope='bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def fully_connected(inputs,\n",
        "                      num_outputs,\n",
        "                      scope,\n",
        "                      use_xavier=True,\n",
        "                      stddev=1e-3,\n",
        "                      weight_decay=0.0,\n",
        "                      activation_fn=tf.nn.relu,\n",
        "                      bn=False,\n",
        "                      bn_decay=None,\n",
        "                      is_training=None):\n",
        "    \"\"\" Fully connected layer with non-linear operation.\n",
        "    \n",
        "    Args:\n",
        "      inputs: 2-D tensor BxN\n",
        "      num_outputs: int\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor of size B x num_outputs.\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      num_input_units = inputs.get_shape()[-1].value\n",
        "      weights = _variable_with_weight_decay('weights',\n",
        "                                            shape=[num_input_units, num_outputs],\n",
        "                                            use_xavier=use_xavier,\n",
        "                                            stddev=stddev,\n",
        "                                            wd=weight_decay)\n",
        "      outputs = tf.matmul(inputs, weights)\n",
        "      biases = _variable_on_cpu('biases', [num_outputs],\n",
        "                              tf.constant_initializer(0.0))\n",
        "      outputs = tf.nn.bias_add(outputs, biases)\n",
        "      \n",
        "      if bn:\n",
        "        outputs = batch_norm_for_fc(outputs, is_training, bn_decay, 'bn')\n",
        "\n",
        "      if activation_fn is not None:\n",
        "        outputs = activation_fn(outputs)\n",
        "      return outputs\n",
        "\n",
        "  @staticmethod\n",
        "  def max_pool2d(inputs,\n",
        "                kernel_size,\n",
        "                scope,\n",
        "                stride=[2, 2],\n",
        "                padding='VALID'):\n",
        "    \"\"\" 2D max pooling.\n",
        "\n",
        "    Args:\n",
        "      inputs: 4-D tensor BxHxWxC\n",
        "      kernel_size: a list of 2 ints\n",
        "      stride: a list of 2 ints\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_h, kernel_w = kernel_size\n",
        "      stride_h, stride_w = stride\n",
        "      outputs = tf.nn.max_pool(inputs,\n",
        "                              ksize=[1, kernel_h, kernel_w, 1],\n",
        "                              strides=[1, stride_h, stride_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=sc.name)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def avg_pool2d(inputs,\n",
        "                kernel_size,\n",
        "                scope,\n",
        "                stride=[2, 2],\n",
        "                padding='VALID'):\n",
        "    \"\"\" 2D avg pooling.\n",
        "\n",
        "    Args:\n",
        "      inputs: 4-D tensor BxHxWxC\n",
        "      kernel_size: a list of 2 ints\n",
        "      stride: a list of 2 ints\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_h, kernel_w = kernel_size\n",
        "      stride_h, stride_w = stride\n",
        "      outputs = tf.nn.avg_pool(inputs,\n",
        "                              ksize=[1, kernel_h, kernel_w, 1],\n",
        "                              strides=[1, stride_h, stride_w, 1],\n",
        "                              padding=padding,\n",
        "                              name=sc.name)\n",
        "      return outputs\n",
        "\n",
        "  @staticmethod\n",
        "  def max_pool3d(inputs,\n",
        "                kernel_size,\n",
        "                scope,\n",
        "                stride=[2, 2, 2],\n",
        "                padding='VALID'):\n",
        "    \"\"\" 3D max pooling.\n",
        "\n",
        "    Args:\n",
        "      inputs: 5-D tensor BxDxHxWxC\n",
        "      kernel_size: a list of 3 ints\n",
        "      stride: a list of 3 ints\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_d, kernel_h, kernel_w = kernel_size\n",
        "      stride_d, stride_h, stride_w = stride\n",
        "      outputs = tf.nn.max_pool3d(inputs,\n",
        "                                ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
        "                                strides=[1, stride_d, stride_h, stride_w, 1],\n",
        "                                padding=padding,\n",
        "                                name=sc.name)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def avg_pool3d(inputs,\n",
        "                kernel_size,\n",
        "                scope,\n",
        "                stride=[2, 2, 2],\n",
        "                padding='VALID'):\n",
        "    \"\"\" 3D avg pooling.\n",
        "\n",
        "    Args:\n",
        "      inputs: 5-D tensor BxDxHxWxC\n",
        "      kernel_size: a list of 3 ints\n",
        "      stride: a list of 3 ints\n",
        "    \n",
        "    Returns:\n",
        "      Variable tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      kernel_d, kernel_h, kernel_w = kernel_size\n",
        "      stride_d, stride_h, stride_w = stride\n",
        "      outputs = tf.nn.avg_pool3d(inputs,\n",
        "                                ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
        "                                strides=[1, stride_d, stride_h, stride_w, 1],\n",
        "                                padding=padding,\n",
        "                                name=sc.name)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_template(inputs, is_training, scope, moments_dims, bn_decay):\n",
        "    \"\"\" Batch normalization on convolutional maps and beyond...\n",
        "    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
        "    \n",
        "    Args:\n",
        "        inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n",
        "        is_training:   boolean tf.Varialbe, true indicates training phase\n",
        "        scope:         string, variable scope\n",
        "        moments_dims:  a list of ints, indicating dimensions for moments calculation\n",
        "        bn_decay:      float or float tensor variable, controling moving average weight\n",
        "    Return:\n",
        "        normed:        batch-normalized maps\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      num_channels = inputs.get_shape()[-1].value\n",
        "      beta = tf.Variable(tf.constant(0.0, shape=[num_channels]),\n",
        "                        name='beta', trainable=True)\n",
        "      gamma = tf.Variable(tf.constant(1.0, shape=[num_channels]),\n",
        "                          name='gamma', trainable=True)\n",
        "      batch_mean, batch_var = tf.nn.moments(inputs, moments_dims, name='moments')\n",
        "      decay = bn_decay if bn_decay is not None else 0.9\n",
        "      ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
        "      # Operator that maintains moving averages of variables.\n",
        "      ema_apply_op = tf.cond(is_training,\n",
        "                            lambda: ema.apply([batch_mean, batch_var]),\n",
        "                            lambda: tf.no_op())\n",
        "      \n",
        "      # Update moving average and return current batch's avg and var.\n",
        "      def mean_var_with_update():\n",
        "        with tf.control_dependencies([ema_apply_op]):\n",
        "          return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "      \n",
        "      # ema.average returns the Variable holding the average of var.\n",
        "      mean, var = tf.cond(is_training,\n",
        "                          mean_var_with_update,\n",
        "                          lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "      normed = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, 1e-3)\n",
        "    return normed\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_for_fc(inputs, is_training, bn_decay, scope):\n",
        "    \"\"\" Batch normalization on FC data.\n",
        "    \n",
        "    Args:\n",
        "        inputs:      Tensor, 2D BxC input\n",
        "        is_training: boolean tf.Varialbe, true indicates training phase\n",
        "        bn_decay:    float or float tensor variable, controling moving average weight\n",
        "        scope:       string, variable scope\n",
        "    Return:\n",
        "        normed:      batch-normalized maps\n",
        "    \"\"\"\n",
        "    return batch_norm_template(inputs, is_training, scope, [0,], bn_decay)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_for_conv1d(inputs, is_training, bn_decay, scope):\n",
        "    \"\"\" Batch normalization on 1D convolutional maps.\n",
        "    \n",
        "    Args:\n",
        "        inputs:      Tensor, 3D BLC input maps\n",
        "        is_training: boolean tf.Varialbe, true indicates training phase\n",
        "        bn_decay:    float or float tensor variable, controling moving average weight\n",
        "        scope:       string, variable scope\n",
        "    Return:\n",
        "        normed:      batch-normalized maps\n",
        "    \"\"\"\n",
        "    return batch_norm_template(inputs, is_training, scope, [0,1], bn_decay)\n",
        "\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_for_conv2d(inputs, is_training, bn_decay, scope):\n",
        "    \"\"\" Batch normalization on 2D convolutional maps.\n",
        "    \n",
        "    Args:\n",
        "        inputs:      Tensor, 4D BHWC input maps\n",
        "        is_training: boolean tf.Varialbe, true indicates training phase\n",
        "        bn_decay:    float or float tensor variable, controling moving average weight\n",
        "        scope:       string, variable scope\n",
        "    Return:\n",
        "        normed:      batch-normalized maps\n",
        "    \"\"\"\n",
        "    return batch_norm_template(inputs, is_training, scope, [0,1,2], bn_decay)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_norm_for_conv3d(inputs, is_training, bn_decay, scope):\n",
        "    \"\"\" Batch normalization on 3D convolutional maps.\n",
        "    \n",
        "    Args:\n",
        "        inputs:      Tensor, 5D BDHWC input maps\n",
        "        is_training: boolean tf.Varialbe, true indicates training phase\n",
        "        bn_decay:    float or float tensor variable, controling moving average weight\n",
        "        scope:       string, variable scope\n",
        "    Return:\n",
        "        normed:      batch-normalized maps\n",
        "    \"\"\"\n",
        "    return batch_norm_template(inputs, is_training, scope, [0,1,2,3], bn_decay)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def dropout(inputs,\n",
        "              is_training,\n",
        "              scope,\n",
        "              keep_prob=0.5,\n",
        "              noise_shape=None):\n",
        "    \"\"\" Dropout layer.\n",
        "\n",
        "    Args:\n",
        "      inputs: tensor\n",
        "      is_training: boolean tf.Variable\n",
        "      scope: string\n",
        "      keep_prob: float in [0,1]\n",
        "      noise_shape: list of ints\n",
        "\n",
        "    Returns:\n",
        "      tensor variable\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope) as sc:\n",
        "      outputs = tf.cond(is_training,\n",
        "                        lambda: tf.nn.dropout(inputs, keep_prob, noise_shape),\n",
        "                        lambda: inputs)\n",
        "      return outputs\n",
        "\n",
        "tf_util = TF_util()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVLyuktonKYg"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy8thGIn8oWZ"
      },
      "source": [
        "# Global constants:\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_POINT = 1024\n",
        "MAX_EPOCH = 250\n",
        "BASE_LEARNING_RATE = 0.001\n",
        "# GPU_INDEX = 0\n",
        "GPU_INDEX = 1\n",
        "MOMENTUM = 0.9\n",
        "OPTIMIZER = 'adam'\n",
        "DECAY_STEP = 20000\n",
        "DECAY_RATE = 0.8\n",
        "\n",
        "MAX_NUM_POINT = 2048\n",
        "# NUM_CLASSES = 40\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "BN_INIT_DECAY = 0.5\n",
        "BN_DECAY_DECAY_RATE = 0.5\n",
        "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
        "BN_DECAY_CLIP = 0.99"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGU1LDoAw7Fl"
      },
      "source": [
        "def even_sample(mesh, n, r):\n",
        "    samples = np.zeros((1, 1))\n",
        "    while samples.shape[0] != n:\n",
        "      samples, sampled_face_idx = sample_surface_even(mesh, n, r)\n",
        "      r /= 10\n",
        "    return samples, sampled_face_idx"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJaq8SM6BlqR"
      },
      "source": [
        "Uncomment the cell bellow if the train and test data where not already generated and saved:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnP3yfcaiWDu"
      },
      "source": [
        "# dir_path = '/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/ModelNet10/ModelNet10/'\n",
        "# classes = os.listdir(dir_path)\n",
        "# train_labels, test_labels = [], []\n",
        "# train_data, test_data = [], []\n",
        "# train_faces, test_faces = [], []\n",
        "\n",
        "# for c_i, c in tqdm(enumerate(classes), total=len(classes)):\n",
        "\n",
        "#   # Train data loading:\n",
        "#   for mesh in tqdm(os.listdir(dir_path + c + '/train')):\n",
        "#     train_labels += [c_i]\n",
        "#     train_mesh = trimesh.load_mesh(dir_path + c + '/train/' + mesh)\n",
        "#     samples, sampled_face_idx = even_sample(train_mesh, NUM_POINT, 1)\n",
        "#     train_data += [samples]\n",
        "#     train_faces += [sampled_face_idx]\n",
        "\n",
        "#   # Test data loading:\n",
        "#   for mesh in tqdm(os.listdir(dir_path + c + '/test')):\n",
        "#     test_labels += [c_i]\n",
        "#     test_mesh = trimesh.load_mesh(dir_path + c + '/test/' + mesh)\n",
        "#     samples, sampled_face_idx = even_sample(test_mesh, NUM_POINT, 1)\n",
        "#     test_data += [samples]\n",
        "#     test_faces += [sampled_face_idx]\n",
        "\n",
        "# # Concatenate the data:\n",
        "# train_data_concatenated = np.concatenate([a[np.newaxis, :, :] for a in train_data], axis=0)\n",
        "# test_data_concatenated = np.concatenate([a[np.newaxis, :, :] for a in test_data], axis=0)\n",
        "\n",
        "# # Save the data:\n",
        "# with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/train_data.npy', 'wb') as train_file:\n",
        "#   np.save(train_file, train_data_concatenated)\n",
        "# with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/test_data.npy', 'wb') as test_file:\n",
        "#   np.save(test_file, test_data_concatenated)\n",
        "\n",
        "# # Concatenate the faces data:\n",
        "# train_faces_concatenated = np.concatenate([a[np.newaxis, :] for a in train_faces], axis=0)\n",
        "# test_faces_concatenated = np.concatenate([a[np.newaxis, :] for a in test_faces], axis=0)\n",
        "\n",
        "# # Save the face data:\n",
        "# with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/train_faces.npy', 'wb') as train_faces_file:\n",
        "#   np.save(train_faces_file, train_faces_concatenated)\n",
        "# with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/test_faces.npy', 'wb') as test_faces_file:\n",
        "#   np.save(test_faces_file, test_faces_concatenated)\n",
        "\n",
        "# # Save the labels data:\n",
        "# with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/train_labels.npy', 'wb') as train_labels_file:\n",
        "#   np.save(train_labels_file, np.array(train_labels))\n",
        "# with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/test_labels.npy', 'wb') as test_labels_file:\n",
        "#   np.save(test_labels_file, np.array(test_labels))\n",
        "\n",
        "# train_data = train_data_concatenated\n",
        "# test_data = test_data_concatenated\n",
        "# train_faces = train_faces_concatenated\n",
        "# test_faces = test_faces_concatenated\n",
        "# train_labels = np.array(train_labels)\n",
        "# test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqf8JjSdBvsq"
      },
      "source": [
        "Run only the cells bellow if the train and test data were generated and saved:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGY59eBTBPcV"
      },
      "source": [
        "# Load the data:\n",
        "with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/train_data.npy', 'rb') as train_file:\n",
        "  train_data = np.load(train_file, allow_pickle=True)\n",
        "with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/test_data.npy', 'rb') as test_file:\n",
        "  test_data = np.load(test_file, allow_pickle=True)\n",
        "\n",
        "# Load the faces data:\n",
        "with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/train_faces.npy', 'rb') as train_faces_file:\n",
        "  train_faces = np.load(train_faces_file, allow_pickle=True)\n",
        "with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/test_faces.npy', 'rb') as test_faces_file:\n",
        "  test_faces = np.load(test_faces_file, allow_pickle=True)\n",
        "\n",
        "# Load the labels:\n",
        "with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/train_labels.npy', 'rb') as train_labels_file:\n",
        "  train_labels = np.load(train_labels_file, allow_pickle=True)\n",
        "with open('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/test_labels.npy', 'rb') as test_labels_file:\n",
        "  test_labels = np.load(test_labels_file, allow_pickle=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUZqttAqD3S-"
      },
      "source": [
        "TRAIN_DATA = train_data\n",
        "TEST_DATA = test_data\n",
        "TRAIN_LABELS = train_labels\n",
        "TEST_LABELS = test_labels\n",
        "TRAIN_FACES = train_faces\n",
        "TEST_FACES = test_faces"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OshcE3Waj1Uv"
      },
      "source": [
        "# Run basic experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pD2lhD2iWdU"
      },
      "source": [
        "## Classic **PointNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovXx0aUTihnX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73IeJRoBiiLF"
      },
      "source": [
        "## Classic **Momenet**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp7jCqFHxQo3"
      },
      "source": [
        "class Momenet():\n",
        "  def placeholder_inputs(batch_size, num_point):\n",
        "      pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 9))\n",
        "      labels_pl = tf.placeholder(tf.int32, shape=(batch_size))\n",
        "      return pointclouds_pl, labels_pl\n",
        "\n",
        "\n",
        "  def get_model(point_cloud, is_training, bn_decay=None):\n",
        "      \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
        "      batch_size = point_cloud.get_shape()[0].value\n",
        "      num_point = point_cloud.get_shape()[1].value\n",
        "      end_points = {}\n",
        "      input_image = tf.expand_dims(point_cloud, -1)\n",
        "      net = 1\n",
        "      # Point functions (MLP implemented as conv2d)\n",
        "      net = tf_util.conv2d(input_image, 64, [1,9], 'conv1',\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          bn_decay=bn_decay)\n",
        "      net = tf_util.conv2d(net, 64, [1,1],\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          scope='conv2', bn_decay=bn_decay)\n",
        "      net = tf_util.conv2d(net, 64, [1,1],\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          scope='conv3', bn_decay=bn_decay)\n",
        "      net = tf_util.conv2d(net, 128, [1,1],\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          scope='conv4', bn_decay=bn_decay)\n",
        "      net = tf_util.conv2d(net, 1024, [1,1],\n",
        "                          padding='VALID', stride=[1,1],\n",
        "                          bn=True, is_training=is_training,\n",
        "                          scope='conv5', bn_decay=bn_decay)\n",
        "\n",
        "      # Symmetric function: max pooling\n",
        "      net = tf_util.max_pool2d(net, [num_point,1],\n",
        "                              padding='VALID', scope='maxpool')\n",
        "      \n",
        "      # MLP on global point cloud vector\n",
        "      net = tf.reshape(net, [batch_size, -1])\n",
        "      net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
        "                                    scope='fc1', bn_decay=bn_decay)\n",
        "      net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
        "                                    scope='fc2', bn_decay=bn_decay)\n",
        "      net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n",
        "                            scope='dp1')\n",
        "      # net = tf_util.fully_connected(net, 40, activation_fn=None, scope='fc3')\n",
        "      net = tf_util.fully_connected(net, 10, activation_fn=None, scope='fc3')\n",
        "\n",
        "      return net, end_points\n",
        "\n",
        "\n",
        "  def get_loss(pred, label, end_points):\n",
        "      \"\"\" pred: B*NUM_CLASSES,\n",
        "          label: B, \"\"\"\n",
        "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
        "      classify_loss = tf.reduce_mean(loss)\n",
        "      tf.summary.scalar('classify loss', classify_loss)\n",
        "      return classify_loss\n",
        "\n",
        "MODEL = Momenet"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_FarEV8Q_9S"
      },
      "source": [
        "LOG_DIR = '/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/Momenet/log'\n",
        "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpHKMaN06cAc",
        "outputId": "f2bdfc29-6eb7-40e1-d33a-3fbde671aa75"
      },
      "source": [
        "# Sanity check that get_model works\n",
        "with tf.Graph().as_default():\n",
        "  inputs = tf.zeros((32, 1024, 9))\n",
        "  outputs = MODEL.get_model(inputs, tf.constant(True))\n",
        "  print(outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor 'fc3/BiasAdd:0' shape=(32, 40) dtype=float32>, {})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "259kAz4Sep-c"
      },
      "source": [
        "def add_moments(data, order=2):\n",
        "  if order == 1:\n",
        "    return data\n",
        "  elif order == 2:\n",
        "    data_moments = np.zeros((np.shape(data)[0], np.shape(data)[1], 9))\n",
        "  elif order == 3:\n",
        "    raise ValueError('3rd order moments are not prepared yet!')\n",
        "  \n",
        "  data_moments[:, :, 0:3] = data\n",
        "  data_moments[:, :, 3] = data_moments[:, :, 0] * data_moments[:, :, 0]\n",
        "  data_moments[:, :, 4] = data_moments[:, :, 1] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 5] = data_moments[:, :, 2] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 6] = data_moments[:, :, 0] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 7] = data_moments[:, :, 0] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 8] = data_moments[:, :, 1] * data_moments[:, :, 2]\n",
        "\n",
        "  return data_moments"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mIEwc6GUFYt"
      },
      "source": [
        "def get_bn_decay(batch):\n",
        "  bn_momentum = tf.train.exponential_decay(\n",
        "                    BN_INIT_DECAY,\n",
        "                    batch*BATCH_SIZE,\n",
        "                    BN_DECAY_DECAY_STEP,\n",
        "                    BN_DECAY_DECAY_RATE,\n",
        "                    staircase=True)\n",
        "  bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
        "  return bn_decay"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hraDbagYpi3r"
      },
      "source": [
        "def log_string(out_str):\n",
        "    LOG_FOUT.write(out_str+'\\n')\n",
        "    LOG_FOUT.flush()\n",
        "    print(out_str)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub-GfCTxn1FV"
      },
      "source": [
        "def get_learning_rate(batch):\n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
        "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
        "                        DECAY_STEP,          # Decay step.\n",
        "                        DECAY_RATE,          # Decay rate.\n",
        "                        staircase=True)\n",
        "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
        "    return learning_rate   "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhnd-77oWjT-"
      },
      "source": [
        "def shuffle_data(data, labels):\n",
        "    \"\"\" Shuffle data and labels.\n",
        "        Input:\n",
        "          data: B,N,... numpy array\n",
        "          label: B,... numpy array\n",
        "        Return:\n",
        "          shuffled data, label and shuffle indices\n",
        "    \"\"\"\n",
        "    idx = np.arange(len(labels))\n",
        "    np.random.shuffle(idx)\n",
        "    return data[idx, ...], labels[idx], idx"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqdNw9JVXP8D"
      },
      "source": [
        "def rotate_point_cloud(batch_data):\n",
        "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
        "        rotation is per shape based along up direction\n",
        "        Input:\n",
        "          BxNx3 array, original batch of point clouds\n",
        "        Return:\n",
        "          BxNx3 array, rotated batch of point clouds\n",
        "    \"\"\"\n",
        "    batch_data = batch_data[:,:,0:3]\n",
        "    rotated_data_x = np.zeros(batch_data.shape, dtype=np.float32)\n",
        "    rotated_data_y = np.zeros(batch_data.shape, dtype=np.float32)\n",
        "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
        "    for k in range(batch_data.shape[0]):\n",
        "        rotation_angle_x = np.random.uniform() * 2 * np.pi\n",
        "        cosval_x = np.cos(rotation_angle_x)\n",
        "        sinval_x = np.sin(rotation_angle_x)\n",
        "\n",
        "        #rotation_angle_y = np.random.uniform() * 2 * np.pi\n",
        "        #cosval_y = np.cos(rotation_angle_y)\n",
        "        #sinval_y = np.sin(rotation_angle_y)\n",
        "        \n",
        "        #rotation_angle_z = np.random.uniform() * 2 * np.pi\n",
        "        #cosval_z = np.cos(rotation_angle_z)\n",
        "        #sinval_z = np.sin(rotation_angle_z)\n",
        "        \n",
        "        rotation_matrix_x = np.array([[1, 0, 0],\n",
        "                                    [0, cosval_x, -sinval_x],\n",
        "                                    [0, sinval_x, cosval_x]])\n",
        "        #rotation_matrix_y = np.array([[cosval_y, 0, sinval_y],\n",
        "        #                                [0, 1, 0],\n",
        "        #                                [-sinval_y, 0, cosval_y]])\n",
        "        #rotation_matrix_z = np.array([[cosval_z, -sinval_z, 0],\n",
        "        #                                [sinval_z, cosval_z, 0],\n",
        "        #                                [0, 0, 1]])\n",
        "        #rot_xy = np.dot(rotation_matrix_y,rotation_matrix_x)\n",
        "        #rotation_matrix = np.dot(rot_xy,rotation_matrix_z)\n",
        "        shape_pc = batch_data[k, ...]\n",
        "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix_x)\n",
        "    rotated_data_=np.zeros(shape=(np.shape(rotated_data)[0],np.shape(rotated_data)[1],9))\n",
        "    rotated_data_ = add_moments(rotated_data)\n",
        "    return rotated_data_\n",
        "    #return rotated_data"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf72m8JLXikE"
      },
      "source": [
        "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n",
        "    \"\"\" Randomly jitter points. jittering is per point.\n",
        "        Input:\n",
        "          BxNx3 array, original batch of point clouds\n",
        "        Return:\n",
        "          BxNx3 array, jittered batch of point clouds\n",
        "    \"\"\"\n",
        "    jittered_data = np.zeros(shape=(np.shape(batch_data)[0],np.shape(batch_data)[1],9))\n",
        "    batch_data_xyz = batch_data[:,:,0:3]\n",
        "    batch_data_moments = batch_data[:,:,3:]\n",
        "    B, N, C = batch_data_xyz.shape\n",
        "    '''B, N, C = batch_data.shape'''\n",
        "    assert(clip > 0)\n",
        "    jittered_data[:,:,0:3] = np.clip(sigma * np.random.randn(B, N, C), -1*clip, clip)\n",
        "    jittered_data[:,:,0:3] += batch_data_xyz\n",
        "    '''jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1*clip, clip)\n",
        "    jittered_data += batch_data_xyz'''\n",
        "    jittered_data[:, :, 3:] = np.clip(sigma**2 * np.random.randn(B, N, 6), -1 * clip, clip)\n",
        "    jittered_data[:, :, 3:] += batch_data_moments\n",
        "    return jittered_data"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX9gACkTqa_z"
      },
      "source": [
        "def train_one_epoch(sess, ops, train_writer):\n",
        "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
        "    is_training = True\n",
        "\n",
        "    current_data, current_label = add_moments(TRAIN_DATA, order=2), TRAIN_LABELS\n",
        "    current_data = current_data[:, 0:NUM_POINT, :]\n",
        "    current_data, current_label, _ = shuffle_data(current_data, np.squeeze(current_label))\n",
        "    current_label = np.squeeze(current_label)\n",
        "\n",
        "    num_batches = current_data.shape[0] // BATCH_SIZE\n",
        "        \n",
        "    total_correct = 0\n",
        "    total_seen = 0\n",
        "    loss_sum = 0\n",
        "       \n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * BATCH_SIZE\n",
        "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
        "\n",
        "        # Augment batched point clouds by rotation and jittering\n",
        "        rotated_data = rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
        "        jittered_data = jitter_point_cloud(rotated_data)\n",
        "        feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
        "                      ops['labels_pl']: current_label[start_idx:end_idx],\n",
        "                      ops['is_training_pl']: is_training,}\n",
        "        \n",
        "        summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
        "            ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
        "        \n",
        "        train_writer.add_summary(summary, step)\n",
        "        pred_val = np.argmax(pred_val, 1)\n",
        "        correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
        "        total_correct += correct\n",
        "        total_seen += BATCH_SIZE\n",
        "        loss_sum += loss_val\n",
        "    \n",
        "    log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
        "    log_string('accuracy: %f' % (total_correct / float(total_seen)))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1lpWm0tkGp1"
      },
      "source": [
        "def eval_one_epoch(sess, ops, test_writer):\n",
        "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
        "    is_training = False\n",
        "    total_correct = 0\n",
        "    total_seen = 0\n",
        "    loss_sum = 0\n",
        "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
        "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
        "    \n",
        "    current_data, current_label = add_moments(TEST_DATA, order=2), TEST_LABELS\n",
        "    current_data = current_data[:, 0:NUM_POINT, :]\n",
        "    current_label = np.squeeze(current_label)\n",
        "        \n",
        "    num_batches = current_data.shape[0] // BATCH_SIZE\n",
        "    \n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * BATCH_SIZE\n",
        "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
        "        \n",
        "        #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "        #run_metadata = tf.RunMetadata()\n",
        "        #rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
        "        #jittered_data = provider.jitter_point_cloud(rotated_data)\n",
        "        \n",
        "        feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
        "                      ops['labels_pl']: current_label[start_idx:end_idx],\n",
        "                      ops['is_training_pl']: is_training}\n",
        "        #feed_dict = {ops['pointclouds_pl']: rotated_data,\n",
        "        #             ops['labels_pl']: current_label[start_idx:end_idx],\n",
        "        #             ops['is_training_pl']: is_training}            \n",
        "        #start = timeit.default_timer()\n",
        "        \n",
        "        summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
        "            ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
        "\n",
        "        #stop = timeit.default_timer()\n",
        "\n",
        "        #print (stop - start)\n",
        "        #options=run_options, run_metadata=run_metadata)\n",
        "        \n",
        "        #tl = timeline.Timeline(run_metadata.step_stats)\n",
        "        #ctf = tl.generate_chrome_trace_format()\n",
        "        #with open('timeline.json', 'w') as f:\n",
        "        #    f.write(ctf)\n",
        "        \n",
        "        pred_val = np.argmax(pred_val, 1)\n",
        "        correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
        "        total_correct += correct\n",
        "        total_seen += BATCH_SIZE\n",
        "        loss_sum += (loss_val*BATCH_SIZE)\n",
        "        for i in range(start_idx, end_idx):\n",
        "            l = current_label[i]\n",
        "            total_seen_class[l] += 1\n",
        "            total_correct_class[l] += (pred_val[i-start_idx] == l)\n",
        "        \n",
        "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen)))\n",
        "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
        "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
        "    return (total_correct / float(total_seen))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc-PQcI9i5tU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865,
          "referenced_widgets": [
            "6dd8bb313e014a3fbb3e0e7b5ccf8679",
            "9c6fd36f004e4edd948499842f81d5ca",
            "823fb77ab266468fad38b8ca28478d95",
            "0a0e8de7e97843e2aa18c69e3da67643",
            "0e5be25907914cc9922a4dc40774ba02",
            "e57002b2fd25444fb6af5d2fa4be9ef0",
            "19915736695540858d9eb2706c878817",
            "ae6288870c874c329c527632eefcfef4"
          ]
        },
        "outputId": "a0cb1226-7a17-489b-a933-cb002a66ec16"
      },
      "source": [
        "def train():\n",
        "  with tf.Graph().as_default():\n",
        "    with tf.device('/gpu:' + str(GPU_INDEX)):\n",
        "      pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
        "      is_training_pl = tf.placeholder(tf.bool, shape=())\n",
        "      print(is_training_pl)\n",
        "      \n",
        "      # Note the global_step=batch parameter to minimize. \n",
        "      # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
        "      batch = tf.Variable(0)\n",
        "      bn_decay = get_bn_decay(batch)\n",
        "      tf.summary.scalar('bn_decay', bn_decay)\n",
        "\n",
        "      # Get model and loss \n",
        "      pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
        "      loss = MODEL.get_loss(pred, labels_pl, end_points)\n",
        "      tf.summary.scalar('loss', loss)\n",
        "\n",
        "      correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n",
        "      accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
        "      tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "      # Get training operator\n",
        "      learning_rate = get_learning_rate(batch)\n",
        "      tf.summary.scalar('learning_rate', learning_rate)\n",
        "      if OPTIMIZER == 'momentum':\n",
        "          optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
        "      elif OPTIMIZER == 'adam':\n",
        "          optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "      train_op = optimizer.minimize(loss, global_step=batch)\n",
        "      \n",
        "      # Add ops to save and restore all the variables.\n",
        "      saver = tf.train.Saver()\n",
        "          \n",
        "      # Create a session\n",
        "      config = tf.ConfigProto()\n",
        "      config.gpu_options.allow_growth = True\n",
        "      config.allow_soft_placement = True\n",
        "      config.log_device_placement = False\n",
        "      sess = tf.Session(config=config)\n",
        "\n",
        "      # Add summary writers\n",
        "      #merged = tf.merge_all_summaries()\n",
        "      merged = tf.summary.merge_all()\n",
        "      train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
        "                                sess.graph)\n",
        "      test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
        "\n",
        "      # Init variables\n",
        "      init = tf.global_variables_initializer()\n",
        "      # To fix the bug introduced in TF 0.12.1 as in\n",
        "      # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
        "      #sess.run(init)\n",
        "      sess.run(init, {is_training_pl: True})\n",
        "\n",
        "      ops = {'pointclouds_pl': pointclouds_pl,\n",
        "              'labels_pl': labels_pl,\n",
        "              'is_training_pl': is_training_pl,\n",
        "              'pred': pred,\n",
        "              'loss': loss,\n",
        "              'train_op': train_op,\n",
        "              'merged': merged,\n",
        "              'step': batch}\n",
        "      best_acc=0\n",
        "      for epoch in tqdm(range(MAX_EPOCH)):\n",
        "          log_string('**** EPOCH %03d ****' % (epoch))\n",
        "          # sys.stdout.flush()\n",
        "            \n",
        "          train_one_epoch(sess, ops, train_writer)\n",
        "          acc = eval_one_epoch(sess, ops, test_writer)\n",
        "          \n",
        "          if acc > best_acc:\n",
        "              save_path = saver.save(sess, os.path.join(LOG_DIR, \"best_model.ckpt\"))\n",
        "              log_string(\"Best Model saved in file: %s\" % save_path)\n",
        "              best_acc = acc\n",
        "          \n",
        "          # Save the variables to disk.\n",
        "          if epoch % 10 == 0:\n",
        "              save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
        "              log_string(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Placeholder_2:0\", shape=(), dtype=bool, device=/device:GPU:0)\n",
            "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dd8bb313e014a3fbb3e0e7b5ccf8679",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "**** EPOCH 000 ****\n",
            "mean loss: 2.178385\n",
            "accuracy: 0.274194\n",
            "eval mean loss: 4.602922\n",
            "eval accuracy: 0.114955\n",
            "eval avg class acc: 0.103000\n",
            "Best Model saved in file: /content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/Momenet/log/best_model.ckpt\n",
            "Model saved in file: /content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/Momenet/log/model.ckpt\n",
            "**** EPOCH 001 ****\n",
            "mean loss: 2.040367\n",
            "accuracy: 0.326613\n",
            "eval mean loss: 3.951823\n",
            "eval accuracy: 0.113839\n",
            "eval avg class acc: 0.102000\n",
            "**** EPOCH 002 ****\n",
            "mean loss: 1.975955\n",
            "accuracy: 0.349798\n",
            "eval mean loss: 3.626220\n",
            "eval accuracy: 0.120536\n",
            "eval avg class acc: 0.108000\n",
            "Best Model saved in file: /content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/Momenet/log/best_model.ckpt\n",
            "**** EPOCH 003 ****\n",
            "mean loss: 1.904908\n",
            "accuracy: 0.376764\n",
            "eval mean loss: 3.872882\n",
            "eval accuracy: 0.112723\n",
            "eval avg class acc: 0.101000\n",
            "**** EPOCH 004 ****\n",
            "mean loss: 1.926176\n",
            "accuracy: 0.378780\n",
            "eval mean loss: 4.554396\n",
            "eval accuracy: 0.112723\n",
            "eval avg class acc: 0.101000\n",
            "**** EPOCH 005 ****\n",
            "mean loss: 1.817390\n",
            "accuracy: 0.406754\n",
            "eval mean loss: 4.278001\n",
            "eval accuracy: 0.111607\n",
            "eval avg class acc: 0.100000\n",
            "**** EPOCH 006 ****\n",
            "mean loss: 1.790258\n",
            "accuracy: 0.420363\n",
            "eval mean loss: 3.574346\n",
            "eval accuracy: 0.112723\n",
            "eval avg class acc: 0.101000\n",
            "**** EPOCH 007 ****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78UDJvSvi6Pe"
      },
      "source": [
        "## $1^{st}$, $2^{nd}$ and $3^{rd}$ order moments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m9TMUU-jbWe"
      },
      "source": [
        "def add_moments(data, order=2):\n",
        "  if order == 1:\n",
        "    return data\n",
        "  elif order == 2:\n",
        "    data_moments = np.zeros((np.shape(data)[0], np.shape(data)[1], 9))\n",
        "  elif order == 3:\n",
        "    raise ValueError('3rd order moments are not prepared yet!')\n",
        "  \n",
        "  data_moments[:, :, 0:3] = data\n",
        "  data_moments[:, :, 3] = data_moments[:, :, 0] * data_moments[:, :, 0]\n",
        "  data_moments[:, :, 4] = data_moments[:, :, 1] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 5] = data_moments[:, :, 2] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 6] = data_moments[:, :, 0] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 7] = data_moments[:, :, 0] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 8] = data_moments[:, :, 1] * data_moments[:, :, 2]\n",
        "\n",
        "  \n",
        "\n",
        "  return data_moments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLmjOJ2CjckY"
      },
      "source": [
        "# Add consistently oriented vertex normals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96T6Qm4BjzeB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsldZFbTj-0r"
      },
      "source": [
        "## Classic **PointNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbt66gNTkGgD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq9R0Jj3kG63"
      },
      "source": [
        "## Classic **Momenet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irBRCORskJbI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGKDTjNIkJ1S"
      },
      "source": [
        "## $1^{st}$, $2^{nd}$ and $3^{rd}$ order moments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNRNny7OkMXP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOzdFxEjkQN4"
      },
      "source": [
        "# Add another geometric prelifting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X48dGnaOkTlS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sr9SPlRkW15"
      },
      "source": [
        "## Basic runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUrPwBSCkZNp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoWfndBXkZz9"
      },
      "source": [
        "## Consistently oriented vertex normals runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfME_OKF33LR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}