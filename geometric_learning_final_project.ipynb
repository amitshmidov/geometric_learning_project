{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geometric_learning_final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5_a7zJA-iDrw",
        "WoI2KClxM6d9"
      ],
      "authorship_tag": "ABX9TyPvpNRqu6U1elPeRh3+LDi+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitshmidov/geometric_learning_project/blob/main/geometric_learning_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_a7zJA-iDrw"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xChOBbx4nStU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1b13c5-2740-4cf9-c04b-3558e2efb8d3"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pyvista as pv\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm.tqdm_notebook import tqdm_notebook as tqdm\n",
        "\n",
        "from typing import Tuple\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoI2KClxM6d9"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spcm39HPNAZC"
      },
      "source": [
        "def read_off(off_file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Read .off files to shared vertex data structure.\n",
        "    Parameters\n",
        "    ----------\n",
        "    off_file_path: str\n",
        "        Path to .off file\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (v, f): tuple\n",
        "        vertices and faces in shared vertex data structure.\n",
        "    \"\"\"\n",
        "    with open(off_file_path, 'r') as off_file:\n",
        "        off_data = off_file.read().split('\\n')\n",
        "    meta_nums = off_data[off_data[0] == 'OFF'].split()\n",
        "    v_num, f_num = int(meta_nums[0]), int(meta_nums[1])\n",
        "    off_data = off_data[1 + (off_data[0] == 'OFF'):]\n",
        "\n",
        "    v = np.array(list(map(lambda s: list(map(lambda c: float(c), s.split()[:3])), off_data[:v_num])))\n",
        "    f = np.array(list(map(lambda s: list(map(lambda c: int(c), s.split()[1:4])), off_data[v_num:v_num + f_num])))\n",
        "    return v, f"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVLyuktonKYg"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnP3yfcaiWDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e073adf-bf77-4b87-e8b5-022d24e2236c"
      },
      "source": [
        "v, f = read_off('/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/ModelNet10/ModelNet10/bed/train/bed_0001.off')\n",
        "f_with_dim = np.hstack((3 * np.ones(f.shape[0]).astype(int)[:, np.newaxis], f))\n",
        "\n",
        "dir_path = '/content/drive/My Drive/Technion/Master/Geometric Learning/Final Project/ModelNet10/ModelNet10'\n",
        "classes = os.listdir(dir_path)\n",
        "train_labels, test_labels = [], []\n",
        "train_data, test_data = [], []\n",
        "for c in tqdm(classes):\n",
        "  # Train data loading:\n",
        "  for mesh in os.listdir(dir_path + c + '/train')\n",
        "    train_labels += [c]\n",
        "    train_data += [read_off(dir_path + c + '/train' + mesh)[0]]\n",
        "\n",
        "  # Test data loading:\n",
        "  for mesh in os.listdir(dir_path + c + '/test')\n",
        "    test_labels += [c]\n",
        "    test_data += [read_off(dir_path + c + '/test' + mesh)[0]]\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bathtub',\n",
              " 'bed',\n",
              " 'chair',\n",
              " 'desk',\n",
              " 'dresser',\n",
              " 'monitor',\n",
              " 'night_stand',\n",
              " 'sofa',\n",
              " 'table',\n",
              " 'toilet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OshcE3Waj1Uv"
      },
      "source": [
        "# Run basic experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pD2lhD2iWdU"
      },
      "source": [
        "## Classic **PointNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovXx0aUTihnX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73IeJRoBiiLF"
      },
      "source": [
        "## Classic **Momenet**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l_Jiyn2dcdv"
      },
      "source": [
        "# Constants:\n",
        "BATCH_SIZE = 32\n",
        "NUN_POINT = 1024\n",
        "MAX_EPOCH = 250\n",
        "BASE_LEARNING_RATE = 0.001\n",
        "GPU_INDEX = 0\n",
        "MOMENTUM = 0.9\n",
        "OPTIMIZER = 'adam'\n",
        "DECAY_STEP = 20000\n",
        "DECAY_RATE = 0.8\n",
        "\n",
        "MAX_NUM_POINT = 2048\n",
        "NUN_CLASSES = 40\n",
        "\n",
        "BN_INIT_DECAY = 0.5\n",
        "BN_DECAY_DECAY_RATE = 0.5\n",
        "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
        "BN_DECAY_CLIP = 0.99"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "259kAz4Sep-c"
      },
      "source": [
        "def add_moments(data, order=2):\n",
        "  if order == 1:\n",
        "    return data\n",
        "  elif order == 2:\n",
        "    data_moments = np.zeros((np.shape(data)[0], np.shape(data)[1], 9))\n",
        "  elif order == 3:\n",
        "    raise ValueError('3rd order moments are not prepared yet!')\n",
        "  \n",
        "  data_moments[:, :, 0:3] = data\n",
        "  data_moments[:, :, 3] = data_moments[:, :, 0] * data_moments[:, :, 0]\n",
        "  data_moments[:, :, 4] = data_moments[:, :, 1] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 5] = data_moments[:, :, 2] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 6] = data_moments[:, :, 0] * data_moments[:, :, 1]\n",
        "  data_moments[:, :, 7] = data_moments[:, :, 0] * data_moments[:, :, 2]\n",
        "  data_moments[:, :, 8] = data_moments[:, :, 1] * data_moments[:, :, 2]\n",
        "\n",
        "  return data_moments"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "65ndNiOAgZRS",
        "outputId": "925e4e64-ec50-4248-eab0-f09d1379221d"
      },
      "source": [
        "add_moments(v)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b39a54539dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madd_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-cf56d58bb2f0>\u001b[0m in \u001b[0;36madd_moments\u001b[0;34m(data, order)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3rd order moments are not prepared yet!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mdata_moments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mdata_moments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_moments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata_moments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mdata_moments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_moments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata_moments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2095,3) into shape (2095,3,3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc-PQcI9i5tU"
      },
      "source": [
        "with tf.Graph().as_default():\n",
        "    pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
        "    is_training_pl = tf.placeholder(tf.bool, shape=())\n",
        "    print(is_training_pl)\n",
        "    \n",
        "    # Note the global_step=batch parameter to minimize. \n",
        "    # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
        "    batch = tf.Variable(0)\n",
        "    bn_decay = get_bn_decay(batch)\n",
        "    tf.summary.scalar('bn_decay', bn_decay)\n",
        "\n",
        "    # Get model and loss \n",
        "    pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
        "    loss = MODEL.get_loss(pred, labels_pl, end_points)\n",
        "    tf.summary.scalar('loss', loss)\n",
        "\n",
        "    correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n",
        "    accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "    # Get training operator\n",
        "    learning_rate = get_learning_rate(batch)\n",
        "    tf.summary.scalar('learning_rate', learning_rate)\n",
        "    if OPTIMIZER == 'momentum':\n",
        "        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
        "    elif OPTIMIZER == 'adam':\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    train_op = optimizer.minimize(loss, global_step=batch)\n",
        "    \n",
        "    # Add ops to save and restore all the variables.\n",
        "    saver = tf.train.Saver()\n",
        "        \n",
        "    # Create a session\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.allow_soft_placement = True\n",
        "    config.log_device_placement = False\n",
        "    sess = tf.Session(config=config)\n",
        "\n",
        "    # Add summary writers\n",
        "    #merged = tf.merge_all_summaries()\n",
        "    merged = tf.summary.merge_all()\n",
        "    train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
        "                              sess.graph)\n",
        "    test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
        "\n",
        "    # Init variables\n",
        "    init = tf.global_variables_initializer()\n",
        "    # To fix the bug introduced in TF 0.12.1 as in\n",
        "    # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
        "    #sess.run(init)\n",
        "    sess.run(init, {is_training_pl: True})\n",
        "\n",
        "    ops = {'pointclouds_pl': pointclouds_pl,\n",
        "            'labels_pl': labels_pl,\n",
        "            'is_training_pl': is_training_pl,\n",
        "            'pred': pred,\n",
        "            'loss': loss,\n",
        "            'train_op': train_op,\n",
        "            'merged': merged,\n",
        "            'step': batch}\n",
        "    best_acc=0\n",
        "    for epoch in range(MAX_EPOCH):\n",
        "        log_string('**** EPOCH %03d ****' % (epoch))\n",
        "        sys.stdout.flush()\n",
        "          \n",
        "        train_one_epoch(sess, ops, train_writer)\n",
        "        acc = eval_one_epoch(sess, ops, test_writer)\n",
        "        \n",
        "        if acc > best_acc:\n",
        "            save_path = saver.save(sess, os.path.join(LOG_DIR, \"best_model.ckpt\"))\n",
        "            log_string(\"Best Model saved in file: %s\" % save_path)\n",
        "            best_acc = acc\n",
        "        \n",
        "        # Save the variables to disk.\n",
        "        if epoch % 10 == 0:\n",
        "            save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
        "            log_string(\"Model saved in file: %s\" % save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78UDJvSvi6Pe"
      },
      "source": [
        "## $1^{st}$, $2^{nd}$ and $3^{rd}$ order moments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m9TMUU-jbWe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLmjOJ2CjckY"
      },
      "source": [
        "# Add consistently oriented vertex normals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96T6Qm4BjzeB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsldZFbTj-0r"
      },
      "source": [
        "## Classic **PointNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbt66gNTkGgD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq9R0Jj3kG63"
      },
      "source": [
        "## Classic **Momenet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irBRCORskJbI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGKDTjNIkJ1S"
      },
      "source": [
        "## $1^{st}$, $2^{nd}$ and $3^{rd}$ order moments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNRNny7OkMXP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOzdFxEjkQN4"
      },
      "source": [
        "# Add another geometric prelifting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X48dGnaOkTlS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sr9SPlRkW15"
      },
      "source": [
        "## Basic runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUrPwBSCkZNp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoWfndBXkZz9"
      },
      "source": [
        "## Consistently oriented vertex normals runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vns4DfnBkgxb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}